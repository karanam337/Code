{"cells":[{"cell_type":"code","source":["sc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af2c26b6-f58d-4438-80a1-d4627cdf8f4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.230.137:44877\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":"<div class=\"ansiout\">Out[1]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.230.137:44877\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["rdd1 = sc.textFile('dbfs:/FileStore/shared_uploads/phanikrishna.karanam@mavs.uta.edu/Movies-1.txt')\nrdd1.take(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62ca53fb-b156-4853-aae0-9f8e48845d75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: [&#39;This is a test file&#39;, &#39;to test the spark input file read&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: [&#39;This is a test file&#39;, &#39;to test the spark input file read&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def upper_case(x):\n  return [i.upper() for i in x]\n\n#rdd1.map(lambda x: x.split()).map(lambda x: [i.upper() for i in x]).collect()\nrdd1.map(lambda x: x.split()).map(upper_case).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d41f7e4-9f4a-489b-960f-9cb1a5b26cfa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: [[&#39;THIS&#39;, &#39;IS&#39;, &#39;A&#39;, &#39;TEST&#39;, &#39;FILE&#39;],\n [&#39;TO&#39;, &#39;TEST&#39;, &#39;THE&#39;, &#39;SPARK&#39;, &#39;INPUT&#39;, &#39;FILE&#39;, &#39;READ&#39;]]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: [[&#39;THIS&#39;, &#39;IS&#39;, &#39;A&#39;, &#39;TEST&#39;, &#39;FILE&#39;],\n [&#39;TO&#39;, &#39;TEST&#39;, &#39;THE&#39;, &#39;SPARK&#39;, &#39;INPUT&#39;, &#39;FILE&#39;, &#39;READ&#39;]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def upper_case(x):\n  return x.upper() \n\nrdd1.flatMap(lambda x: x.split()).map(upper_case).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a74d5193-7f05-41c1-9d84-01d944acd432"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[10]: [&#39;THIS&#39;,\n &#39;IS&#39;,\n &#39;A&#39;,\n &#39;TEST&#39;,\n &#39;FILE&#39;,\n &#39;TO&#39;,\n &#39;TEST&#39;,\n &#39;THE&#39;,\n &#39;SPARK&#39;,\n &#39;INPUT&#39;,\n &#39;FILE&#39;,\n &#39;READ&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [&#39;THIS&#39;,\n &#39;IS&#39;,\n &#39;A&#39;,\n &#39;TEST&#39;,\n &#39;FILE&#39;,\n &#39;TO&#39;,\n &#39;TEST&#39;,\n &#39;THE&#39;,\n &#39;SPARK&#39;,\n &#39;INPUT&#39;,\n &#39;FILE&#39;,\n &#39;READ&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a3ea526-fb98-49bb-b2de-55f9ade571d8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[46]: 2</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: 2</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.map(lambda x: x.split()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"691275ac-173e-4e82-a209-bd751d1e4aa4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: [[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;, &#39;file&#39;],\n [&#39;to&#39;, &#39;test&#39;, &#39;the&#39;, &#39;spark&#39;, &#39;input&#39;, &#39;file&#39;, &#39;read&#39;]]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: [[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;, &#39;file&#39;],\n [&#39;to&#39;, &#39;test&#39;, &#39;the&#39;, &#39;spark&#39;, &#39;input&#39;, &#39;file&#39;, &#39;read&#39;]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.flatMap(lambda x: x.split()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e713b430-3bb0-40de-91a6-742e09c12e60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[20]: [&#39;This&#39;,\n &#39;is&#39;,\n &#39;a&#39;,\n &#39;test&#39;,\n &#39;file&#39;,\n &#39;to&#39;,\n &#39;test&#39;,\n &#39;the&#39;,\n &#39;spark&#39;,\n &#39;input&#39;,\n &#39;file&#39;,\n &#39;read&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: [&#39;This&#39;,\n &#39;is&#39;,\n &#39;a&#39;,\n &#39;test&#39;,\n &#39;file&#39;,\n &#39;to&#39;,\n &#39;test&#39;,\n &#39;the&#39;,\n &#39;spark&#39;,\n &#39;input&#39;,\n &#39;file&#39;,\n &#39;read&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["wc = rdd1.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y) \nwc.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80238ba9-5b7e-4a85-a195-64176f668afd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: [(&#39;is&#39;, 1),\n (&#39;test&#39;, 2),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;file&#39;, 2),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: [(&#39;is&#39;, 1),\n (&#39;test&#39;, 2),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;file&#39;, 2),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from operator import add\nwc = rdd1.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(add) \nwc.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e5e1f17-cf4d-43fc-ac7e-3498267bc803"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[12]: [(&#39;is&#39;, 1),\n (&#39;test&#39;, 2),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;file&#39;, 2),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [(&#39;is&#39;, 1),\n (&#39;test&#39;, 2),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;file&#39;, 2),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_values(x, y):\n  return x + y\n\nwc = rdd1.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(add_values) \nwc.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e81f1fe-b021-46ee-821c-30e1adcffb42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[13]: [(&#39;is&#39;, 1),\n (&#39;test&#39;, 2),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;file&#39;, 2),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [(&#39;is&#39;, 1),\n (&#39;test&#39;, 2),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;file&#39;, 2),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_values(x):\n  return x[0] + x[1]\n\nwc.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"168f64b4-1f40-4455-bdc5-a9bf2a041000"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-631672669312139&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> wc <span class=\"ansi-blue-fg\">=</span> rdd1<span class=\"ansi-blue-fg\">.</span>flatMap<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>reduceByKey<span class=\"ansi-blue-fg\">(</span>add_values<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>wc<span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 30, ip-10-172-255-79.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: add_values() takes 1 positional argument but 2 were given&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 644, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 425, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1977, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: add_values() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;TypeError: add_values() takes 1 positional argument but 2 were given&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 644, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 425, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1977, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: add_values() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 30, ip-10-172-255-79.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: add_values() takes 1 positional argument but 2 were given&#39;. Full traceback below:","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-631672669312139&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> wc <span class=\"ansi-blue-fg\">=</span> rdd1<span class=\"ansi-blue-fg\">.</span>flatMap<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>reduceByKey<span class=\"ansi-blue-fg\">(</span>add_values<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>wc<span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 30, ip-10-172-255-79.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: add_values() takes 1 positional argument but 2 were given&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 644, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 425, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1977, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: add_values() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;TypeError: add_values() takes 1 positional argument but 2 were given&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 644, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2627, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 425, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1977, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: add_values() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["wc.sortBy(lambda x: x[1], ascending = False).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9058545-ce4f-4d2b-b47c-98bdeb97778a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[63]: [(&#39;test&#39;, 2),\n (&#39;file&#39;, 2),\n (&#39;is&#39;, 1),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[63]: [(&#39;test&#39;, 2),\n (&#39;file&#39;, 2),\n (&#39;is&#39;, 1),\n (&#39;input&#39;, 1),\n (&#39;read&#39;, 1),\n (&#39;This&#39;, 1),\n (&#39;a&#39;, 1),\n (&#39;to&#39;, 1),\n (&#39;the&#39;, 1),\n (&#39;spark&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["wc = rdd1.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).countByKey() \nprint(wc) # countByKey() is returning a collection object -> like a dict. reduceByKey(lambda x, y: x + y) returns tuple of word and counts as rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd0fc4eb-eeb6-49e2-bd0a-460577606eaa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;This&#39;: 1, &#39;is&#39;: 1, &#39;a&#39;: 1, &#39;test&#39;: 2, &#39;file&#39;: 2, &#39;to&#39;: 1, &#39;the&#39;: 1, &#39;spark&#39;: 1, &#39;input&#39;: 1, &#39;read&#39;: 1})\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;This&#39;: 1, &#39;is&#39;: 1, &#39;a&#39;: 1, &#39;test&#39;: 2, &#39;file&#39;: 2, &#39;to&#39;: 1, &#39;the&#39;: 1, &#39;spark&#39;: 1, &#39;input&#39;: 1, &#39;read&#39;: 1})\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['apple', 'banana', 'fig', 'jamoon','apple','orange']\nwords_rdd = sc.parallelize(alist)\nwords_rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5306169-f2bf-455d-9e4e-b88df189a46e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: ParallelCollectionRDD[20] at readRDDFromInputStream at PythonRDD.scala:413</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: ParallelCollectionRDD[20] at readRDDFromInputStream at PythonRDD.scala:413</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.first()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60b21ce4-c5bc-4dcb-9eff-8e97e3e3a9db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: &#39;apple&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: &#39;apple&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81aea0ca-df2b-495c-9a56-79f02b0e27ca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["words_rdd.take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee9eaf38-0ae3-4301-96bb-bcad708ed322"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[13]: [&#39;apple&#39;, &#39;banana&#39;, &#39;fig&#39;, &#39;jamoon&#39;, &#39;apple&#39;, &#39;orange&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [&#39;apple&#39;, &#39;banana&#39;, &#39;fig&#39;, &#39;jamoon&#39;, &#39;apple&#39;, &#39;orange&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.map(lambda x: x.upper()).take(10)\n# Transformation operation doesnt execute and return a result immediately. Example is map, filter, reduce\n# Action will execute the DAG and return the result. Example is take, count, collect"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b46b479-2ea1-4f35-87e8-722b1bb3b591"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[16]: [&#39;APPLE&#39;, &#39;BANANA&#39;, &#39;FIG&#39;, &#39;JAMOON&#39;, &#39;APPLE&#39;, &#39;ORANGE&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: [&#39;APPLE&#39;, &#39;BANANA&#39;, &#39;FIG&#39;, &#39;JAMOON&#39;, &#39;APPLE&#39;, &#39;ORANGE&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.map(lambda x: x.upper()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88a6941a-4f65-48d0-8192-417aa62a8d01"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[19]: [&#39;APPLE&#39;, &#39;BANANA&#39;, &#39;FIG&#39;, &#39;JAMOON&#39;, &#39;APPLE&#39;, &#39;ORANGE&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: [&#39;APPLE&#39;, &#39;BANANA&#39;, &#39;FIG&#39;, &#39;JAMOON&#39;, &#39;APPLE&#39;, &#39;ORANGE&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c53db70-0c27-4f9a-8f86-5e31bf6b2d7b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[21]: [&#39;apple&#39;, &#39;banana&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[21]: [&#39;apple&#39;, &#39;banana&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.map(lambda x: (x, 1)).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5ea50a0-2452-4b8d-b746-8030e7b3323e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[23]: [(&#39;apple&#39;, 1),\n (&#39;banana&#39;, 1),\n (&#39;fig&#39;, 1),\n (&#39;jamoon&#39;, 1),\n (&#39;apple&#39;, 1),\n (&#39;orange&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: [(&#39;apple&#39;, 1),\n (&#39;banana&#39;, 1),\n (&#39;fig&#39;, 1),\n (&#39;jamoon&#39;, 1),\n (&#39;apple&#39;, 1),\n (&#39;orange&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb0dd42f-6bdf-4acc-8248-5dc771083895"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: [(&#39;orange&#39;, 1), (&#39;apple&#39;, 2), (&#39;fig&#39;, 1), (&#39;jamoon&#39;, 1), (&#39;banana&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: [(&#39;orange&#39;, 1), (&#39;apple&#39;, 2), (&#39;fig&#39;, 1), (&#39;jamoon&#39;, 1), (&#39;banana&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['apple', 'banana', 'fig', 'jamoon','apple','orange']\nrdd = sc.parallelize(alist)\nresult = rdd.map(lambda x: x.upper()).filter(lambda x: x[0] in 'AEIOU').take(10)\nprint(type(result)) # note that when we use an action and assign to variable, output is not stored as rdd\nresult"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48354cd6-dd79-4a6c-9521-ad9f29e4f651"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\nOut[35]: [&#39;APPLE&#39;, &#39;APPLE&#39;, &#39;ORANGE&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\nOut[35]: [&#39;APPLE&#39;, &#39;APPLE&#39;, &#39;ORANGE&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['apple', 'banana', 'fig', 'jamoon','apple','orange', 'Banana', 'Orange', 'Apple', 'Fig']\nrdd = sc.parallelize(alist)\nresult = rdd.filter(lambda x: x[0] in 'AEIOUaeiou').take(10)\nprint(type(result)) # note that when we use an action and assign to variable, output is not stored as rdd\nresult"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c6b1107-9067-474a-adc2-081f62aef975"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\nOut[15]: [&#39;apple&#39;, &#39;apple&#39;, &#39;orange&#39;, &#39;Orange&#39;, &#39;Apple&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\nOut[15]: [&#39;apple&#39;, &#39;apple&#39;, &#39;orange&#39;, &#39;Orange&#39;, &#39;Apple&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['apple', 'banana', 'fig', 'jamoon','apple','orange']\nrdd = sc.parallelize(alist)\nresult = rdd.map(lambda x: x.upper()).filter(lambda x: x[0] in 'AEIOU')\nprint(type(result))\nprint(result.toDebugString()) # to view lineage of RDDs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eb55280-7534-4f71-b939-dc236728b704"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.PipelinedRDD&#39;&gt;\nb&#39;(8) PythonRDD[102] at RDD at PythonRDD.scala:58 []\\n |  ParallelCollectionRDD[101] at readRDDFromInputStream at PythonRDD.scala:413 []&#39;\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.PipelinedRDD&#39;&gt;\nb&#39;(8) PythonRDD[102] at RDD at PythonRDD.scala:58 []\\n |  ParallelCollectionRDD[101] at readRDDFromInputStream at PythonRDD.scala:413 []&#39;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['apple', 'banana', 'fig', 'jamoon','apple','orange', 'Banana', 'Orange', 'Apple', 'Fig']\nrdd = sc.parallelize(alist)\nresult = rdd.map(lambda x: x.lower()).filter(lambda x: x[0] in 'aeiou').map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()\nprint(type(result)) # note that when we use an action and assign to variable, output is not stored as rdd\nresult"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b68d3cb-a4fb-4143-9106-9f64c70582bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\nOut[51]: [(&#39;orange&#39;, 2), (&#39;apple&#39;, 3)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\nOut[51]: [(&#39;orange&#39;, 2), (&#39;apple&#39;, 3)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["temp = ['apple', 'banana', 'fig', 'jamoon','apple','orange']\ntemp.map(lambda x: x.upper()).filter(lambda x: x[0] in 'AEIOU').take(10) # you need to parallelize to use the actions and transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd520335-af5f-47e6-ba5f-169d3f718b25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-677408205899765&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> temp <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;apple&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;banana&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;fig&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;jamoon&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;apple&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;orange&#39;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>temp<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>upper<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-green-fg\">in</span> <span class=\"ansi-blue-fg\">&#39;AEIOU&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\"># you need to parallelize to use the actions and transformations</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;list&#39; object has no attribute &#39;map&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">AttributeError</span>: &#39;list&#39; object has no attribute &#39;map&#39;","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-677408205899765&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> temp <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;apple&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;banana&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;fig&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;jamoon&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;apple&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;orange&#39;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>temp<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">.</span>upper<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-green-fg\">in</span> <span class=\"ansi-blue-fg\">&#39;AEIOU&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\"># you need to parallelize to use the actions and transformations</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;list&#39; object has no attribute &#39;map&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["a = [1,2,3,4]\nb = [5,6,7,8]\n\nrdd1 = sc.parallelize(a)\nrdd2 = sc.parallelize(b)\n\nrdd1.zip(rdd2).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04e6f20f-0778-45e2-97cc-4abbd7e95393"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[66]: [(1, 5), (2, 6), (3, 7), (4, 8)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[66]: [(1, 5), (2, 6), (3, 7), (4, 8)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.mean()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cf59b48-a886-4fc0-9833-2544dd35da7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[67]: 2.5</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[67]: 2.5</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.top(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7319faa6-9785-413d-97e7-33d792cf68d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: [4]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: [4]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.stats()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8069c4f2-0c63-4c68-95e9-f2d99781146a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: (count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: (count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1.variance()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14575201-c441-4e42-8c7c-6ed0da3da16c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: 1.25</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: 1.25</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['user1\\tphani','user2\\tphani1','user3\\tphani3','user4\\tphani4']\nrdd1 = sc.parallelize(alist)\nrdd1.map(lambda x: x.split('\\t')).map(lambda x: (x[0], x[1])).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a24b06b-f939-4790-a08b-9081b12c515f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[26]: [(&#39;user1&#39;, &#39;phani&#39;),\n (&#39;user2&#39;, &#39;phani1&#39;),\n (&#39;user3&#39;, &#39;phani3&#39;),\n (&#39;user4&#39;, &#39;phani4&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[26]: [(&#39;user1&#39;, &#39;phani&#39;),\n (&#39;user2&#39;, &#39;phani1&#39;),\n (&#39;user3&#39;, &#39;phani3&#39;),\n (&#39;user4&#39;, &#39;phani4&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['75039 45.9 -45.6', '75041 25.9 -15.6', '75062 43.9 -35.6']\nrdd1 = sc.parallelize(alist)\nrdd1.keyBy(lambda x: x.split(' ')[0]).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"462e743d-de2c-4728-9b2e-c822d56e2cb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[27]: [(&#39;75039&#39;, &#39;75039 45.9 -45.6&#39;),\n (&#39;75041&#39;, &#39;75041 25.9 -15.6&#39;),\n (&#39;75062&#39;, &#39;75062 43.9 -35.6&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: [(&#39;75039&#39;, &#39;75039 45.9 -45.6&#39;),\n (&#39;75041&#39;, &#39;75041 25.9 -15.6&#39;),\n (&#39;75062&#39;, &#39;75062 43.9 -35.6&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['75039 45.9 -45.6', '75041 25.9 -15.6', '75062 43.9 -35.6']\nrdd1 = sc.parallelize(alist)\nrdd1.map(lambda x: x.split(' ')).map(lambda x: (int(x[0]), (float(x[1]), float(x[2])))).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9da48680-b2fe-46e8-9142-4324f8792725"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: [(75039, (45.9, -45.6)), (75041, (25.9, -15.6)), (75062, (43.9, -35.6))]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: [(75039, (45.9, -45.6)), (75041, (25.9, -15.6)), (75062, (43.9, -35.6))]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist = ['75039 45.9 -45.6', '75041 25.9 -15.6', '75062 43.9 -35.6']\nrdd1 = sc.parallelize(alist)\nrdd1.map(lambda x: x.split()).map(lambda x: (x[0], (x[1], x[2]))).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"092d4e29-f59e-445b-86d9-4602bcbae72e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[34]: [(&#39;75039&#39;, (&#39;45.9&#39;, &#39;-45.6&#39;)),\n (&#39;75041&#39;, (&#39;25.9&#39;, &#39;-15.6&#39;)),\n (&#39;75062&#39;, (&#39;43.9&#39;, &#39;-35.6&#39;))]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: [(&#39;75039&#39;, (&#39;45.9&#39;, &#39;-45.6&#39;)),\n (&#39;75041&#39;, (&#39;25.9&#39;, &#39;-15.6&#39;)),\n (&#39;75062&#39;, (&#39;43.9&#39;, &#39;-35.6&#39;))]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["alist=['00001\\tsku010:sku933:sku022','00002\\tsku090:sku133','00003\\tsku410:sku433:sku122']\nrdd1 = sc.parallelize(alist)\nrdd1.map(lambda x: x.split('\\t')) \\\n    .map(lambda x: (x[0], x[1]))  \\\n    .flatMapValues(lambda x: x.split(':')) \\\n    .collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb9b9d14-1da3-4124-bed0-1918a4566556"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[42]: [(&#39;00001&#39;, &#39;sku010&#39;),\n (&#39;00001&#39;, &#39;sku933&#39;),\n (&#39;00001&#39;, &#39;sku022&#39;),\n (&#39;00002&#39;, &#39;sku090&#39;),\n (&#39;00002&#39;, &#39;sku133&#39;),\n (&#39;00003&#39;, &#39;sku410&#39;),\n (&#39;00003&#39;, &#39;sku433&#39;),\n (&#39;00003&#39;, &#39;sku122&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[42]: [(&#39;00001&#39;, &#39;sku010&#39;),\n (&#39;00001&#39;, &#39;sku933&#39;),\n (&#39;00001&#39;, &#39;sku022&#39;),\n (&#39;00002&#39;, &#39;sku090&#39;),\n (&#39;00002&#39;, &#39;sku133&#39;),\n (&#39;00003&#39;, &#39;sku410&#39;),\n (&#39;00003&#39;, &#39;sku433&#39;),\n (&#39;00003&#39;, &#39;sku122&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f76e620b-17ae-4799-97b0-b7c8682090fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.225.75:42646\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":"<div class=\"ansiout\">Out[43]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.225.75:42646\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["a = [(1, 'abc'), (2, 'cde'),(3, 'fgh')]\nrdd = sc.parallelize(a)\n\nprint(rdd.keys().take(10))\nprint(rdd.values().take(10))\nprint(rdd.lookup(3))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fed93e3b-8812-4046-b53a-aa3299cb5eb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[1, 2, 3]\n[&#39;abc&#39;, &#39;cde&#39;, &#39;fgh&#39;]\n[&#39;fgh&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[1, 2, 3]\n[&#39;abc&#39;, &#39;cde&#39;, &#39;fgh&#39;]\n[&#39;fgh&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd.values().take(10)\nprint(type(result))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f03d5e4-5bd4-4716-861b-4121eb5cb0a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.textFile('/FileStore/tables/purchases_txt.txt') \\\n        .filter(lambda x: not x.startswith('D')) \\\n        .map(lambda x: x.split('\\t'))\n        \nrdd1 = rdd.map(lambda x: (x[2], float(x[4]))) \\\n          .reduceByKey(lambda x, y: x + y)\n\nprint(rdd1.collect())\n\nrdd2 = rdd.map(lambda x: (x[2], x[5])) \\\n          .distinct() \\\n          .groupByKey() \\\n          .mapValues(lambda x: list(x))\n\nprint(rdd2.collect())\n\nrdd3 = rdd.filter(lambda x: x[2] == 'Austin') \\\n          .map(lambda x: ((x[0], x[2]), float(x[4]))) \\\n          .reduceByKey(lambda x, y: x + y) \\\n          .sortByKey()\n\nprint(rdd3.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b10e5c1-2c47-4604-9e11-c2a029b4fada"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;Omaha&#39;, 235.63), (&#39;Stockton&#39;, 247.18), (&#39;Pittsburgh&#39;, 493.51), (&#39;Austin&#39;, 379.6)]\n[(&#39;Omaha&#39;, [&#39;MasterCard&#39;]), (&#39;Stockton&#39;, [&#39;MasterCard&#39;]), (&#39;Austin&#39;, [&#39;Visa&#39;]), (&#39;Pittsburgh&#39;, [&#39;Discover&#39;])]\n[((&#39;1/1/12&#39;, &#39;Austin&#39;), 379.6)]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;Omaha&#39;, 235.63), (&#39;Stockton&#39;, 247.18), (&#39;Pittsburgh&#39;, 493.51), (&#39;Austin&#39;, 379.6)]\n[(&#39;Omaha&#39;, [&#39;MasterCard&#39;]), (&#39;Stockton&#39;, [&#39;MasterCard&#39;]), (&#39;Austin&#39;, [&#39;Visa&#39;]), (&#39;Pittsburgh&#39;, [&#39;Discover&#39;])]\n[((&#39;1/1/12&#39;, &#39;Austin&#39;), 379.6)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.textFile('/FileStore/tables/shakespeare_txt.txt') \\\n        .map(lambda x: x.lower()) \\\n        .flatMap(lambda x: x.split()) \\\n        .filter(lambda x: x[0] in 'aeiou') \\\n        .map(lambda x: (x, 1)) \\\n        .reduceByKey(lambda x, y: x + y) \\\n        .sortBy(lambda x: x[1], ascending = False)\n\nrdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61ed45aa-b916-44e9-a95b-39364b6a59b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[65]: [(&#39;of&#39;, 10),\n (&#39;and&#39;, 8),\n (&#39;in&#39;, 3),\n (&#39;is&#39;, 3),\n (&#39;if&#39;, 3),\n (&#39;a&#39;, 2),\n (&#39;all&#39;, 2),\n (&#39;art&#39;, 2),\n (&#34;ask&#39;d&#34;, 1),\n (&#39;own&#39;, 1),\n (&#39;an&#39;, 1),\n (&#39;use,&#39;, 1),\n (&#34;excuse,&#39;&#34;, 1),\n (&#39;old,&#39;, 1),\n (&#39;iii.&#39;, 1),\n (&#39;another;&#39;, 1),\n (&#39;unbless&#39;, 1),\n (&#34;unear&#39;d&#34;, 1),\n (&#39;april&#39;, 1),\n (&#39;image&#39;, 1),\n (&#39;iv.&#39;, 1),\n (&#39;upon&#39;, 1),\n (&#39;are&#39;, 1),\n (&#39;use&#39;, 1),\n (&#39;alone,&#39;, 1),\n (&#39;unused&#39;, 1),\n (&#39;used,&#39;, 1),\n (&#39;executor&#39;, 1),\n (&#39;on&#39;, 1),\n (&#39;eyes,&#39;, 1),\n (&#39;all-eating&#39;, 1),\n (&#39;answer&#39;, 1),\n (&#39;old&#39;, 1),\n (&#39;it&#39;, 1),\n (&#39;or&#39;, 1),\n (&#39;age&#39;, 1),\n (&#39;unthrifty&#39;, 1),\n (&#39;abuse&#39;, 1),\n (&#39;usurer,&#39;, 1),\n (&#39;acceptable&#39;, 1),\n (&#39;audit&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[65]: [(&#39;of&#39;, 10),\n (&#39;and&#39;, 8),\n (&#39;in&#39;, 3),\n (&#39;is&#39;, 3),\n (&#39;if&#39;, 3),\n (&#39;a&#39;, 2),\n (&#39;all&#39;, 2),\n (&#39;art&#39;, 2),\n (&#34;ask&#39;d&#34;, 1),\n (&#39;own&#39;, 1),\n (&#39;an&#39;, 1),\n (&#39;use,&#39;, 1),\n (&#34;excuse,&#39;&#34;, 1),\n (&#39;old,&#39;, 1),\n (&#39;iii.&#39;, 1),\n (&#39;another;&#39;, 1),\n (&#39;unbless&#39;, 1),\n (&#34;unear&#39;d&#34;, 1),\n (&#39;april&#39;, 1),\n (&#39;image&#39;, 1),\n (&#39;iv.&#39;, 1),\n (&#39;upon&#39;, 1),\n (&#39;are&#39;, 1),\n (&#39;use&#39;, 1),\n (&#39;alone,&#39;, 1),\n (&#39;unused&#39;, 1),\n (&#39;used,&#39;, 1),\n (&#39;executor&#39;, 1),\n (&#39;on&#39;, 1),\n (&#39;eyes,&#39;, 1),\n (&#39;all-eating&#39;, 1),\n (&#39;answer&#39;, 1),\n (&#39;old&#39;, 1),\n (&#39;it&#39;, 1),\n (&#39;or&#39;, 1),\n (&#39;age&#39;, 1),\n (&#39;unthrifty&#39;, 1),\n (&#39;abuse&#39;, 1),\n (&#39;usurer,&#39;, 1),\n (&#39;acceptable&#39;, 1),\n (&#39;audit&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e408f855-2a1b-440b-9acf-f08b202f0e71"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[((&#39;1/1/12&#39;, &#39;Austin&#39;), 379.6)]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[((&#39;1/1/12&#39;, &#39;Austin&#39;), 379.6)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["a = [('DFW', 100), ('IAD', 50), ('DFW', 40), ('IAD', 45), ('del', 60), ('DFW', 30)]\nrdd = sc.parallelize(a)\n\nrdd.groupByKey().map(lambda x: (x[0], '%.2f' %(sum(x[1]) / len(x[1])))).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cd5ba6b-f4f7-4e80-bf0f-9af6779c9dd2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[143]: [(&#39;DFW&#39;, &#39;56.67&#39;), (&#39;del&#39;, &#39;60.00&#39;), (&#39;IAD&#39;, &#39;47.50&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[143]: [(&#39;DFW&#39;, &#39;56.67&#39;), (&#39;del&#39;, &#39;60.00&#39;), (&#39;IAD&#39;, &#39;47.50&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.groupByKey().mapValues(lambda x: '%.2f' %(sum(list(x)) / len(list(x)))).collect() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e24c889-14fc-44a3-9afc-f5bba338a98c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[145]: [(&#39;DFW&#39;, &#39;56.67&#39;), (&#39;del&#39;, &#39;60.00&#39;), (&#39;IAD&#39;, &#39;47.50&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[145]: [(&#39;DFW&#39;, &#39;56.67&#39;), (&#39;del&#39;, &#39;60.00&#39;), (&#39;IAD&#39;, &#39;47.50&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.reduceByKey(lambda x, y: (x + y) / 2).collect() # this is incorrect for calculating average"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86894df4-0824-49a5-ac7a-5ad4877b08a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[148]: [(&#39;DFW&#39;, 50.0), (&#39;del&#39;, 60), (&#39;IAD&#39;, 47.5)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[148]: [(&#39;DFW&#39;, 50.0), (&#39;del&#39;, 60), (&#39;IAD&#39;, 47.5)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["a = [100, 40, 30]\n\nfloat(sum(a) / len(a))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1660c75-1048-4212-8f85-aa1a69fa45a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[87]: 56.666666666666664</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[87]: 56.666666666666664</div>"]}}],"execution_count":0},{"cell_type":"code","source":["aList = [1,2,3,1,1,1,2,3,4,2,2,2,2,2,5,2,2,1]\nrdd = sc.parallelize(aList)\nrdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a48a5536-5b50-4649-b3c5-ea721e486f8f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[96]: [1, 2, 3, 1, 1]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[96]: [1, 2, 3, 1, 1]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.sortBy(lambda x: -x).take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db570b7e-0e44-4d7a-bfff-d37fb7c99d86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[97]: [5, 4, 3, 3, 2]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[97]: [5, 4, 3, 3, 2]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.sortBy(lambda x: x, ascending = False).take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbce2c29-3c90-495a-8028-029820d64f6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[98]: [5, 4, 3, 3, 2]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[98]: [5, 4, 3, 3, 2]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["a =  [('1', 'P.G. Wodehouse'), ('2', 'Arthur Conan Doyle'), ('3', 'P.G. Wodehouse')]\nb = [('1', 'The Return of Jeeves'), ('2', 'The Sign of Four'), ('3', 'Pigs Have Wings')]\n\nrdd1 = sc.parallelize(a)\nrdd2 = sc.parallelize(b)\n\nrdd1.join(rdd2).map(lambda x: x[1]).groupByKey().mapValues(lambda x: list(x)).sortByKey().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9f9702c-48a1-4fdf-9fc0-8da1ba770a19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[112]: [(&#39;Arthur Conan Doyle&#39;, [&#39;The Sign of Four&#39;]),\n (&#39;P.G. Wodehouse&#39;, [&#39;The Return of Jeeves&#39;, &#39;Pigs Have Wings&#39;])]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[112]: [(&#39;Arthur Conan Doyle&#39;, [&#39;The Sign of Four&#39;]),\n (&#39;P.G. Wodehouse&#39;, [&#39;The Return of Jeeves&#39;, &#39;Pigs Have Wings&#39;])]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["a = [1,2,3,4,5]\nrdd = sc.parallelize(a)\n\nresult = rdd.reduce(lambda x, y: x*y)\nresult"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56ec8303-8a37-473a-b806-19f5efe8ce8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[113]: 120</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[113]: 120</div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd.map(lambda x, y: x * y)\nresult.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d667db1d-e702-42e5-bd2a-924b4b485f7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2233775374729301&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> result <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">:</span> x <span class=\"ansi-blue-fg\">*</span> y<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>result<span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 221.0 failed 1 times, most recent failure: Lost task 1.0 in stage 221.0 (TID 1118, ip-10-172-255-79.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor412.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;TypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 221.0 failed 1 times, most recent failure: Lost task 1.0 in stage 221.0 (TID 1118, ip-10-172-255-79.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;&#39;. Full traceback below:","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2233775374729301&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> result <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">:</span> x <span class=\"ansi-blue-fg\">*</span> y<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>result<span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 221.0 failed 1 times, most recent failure: Lost task 1.0 in stage 221.0 (TID 1118, ip-10-172-255-79.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor412.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;TypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;&#39;. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\nTypeError: &lt;lambda&gt;() missing 1 required positional argument: &#39;y&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd.reduce(lambda x,y: x+y)\nresult"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2511f58b-c774-466b-9cd4-fdcbd5ade96b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[119]: 15</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[119]: 15</div>"]}}],"execution_count":0},{"cell_type":"code","source":["paired_RDD = sc.parallelize([(\"Mon\", 200.00), (\"Tue\", 1215.50), (\"Mon\", 300.25),(\"Wed\", 100.00),(\"Mon\", 100.00)])\n#get total sales by day\nsales_by_day = paired_RDD.reduceByKey(lambda x, y: x + y)\nsales_by_day.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa53e14d-02c7-4a9f-be24-3561a558d98b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[120]: [(&#39;Wed&#39;, 100.0), (&#39;Mon&#39;, 600.25), (&#39;Tue&#39;, 1215.5)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[120]: [(&#39;Wed&#39;, 100.0), (&#39;Mon&#39;, 600.25), (&#39;Tue&#39;, 1215.5)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["grouped_RDD = paired_RDD.groupByKey()\ngrouped_RDD.collect() #note that the value will be an iterator"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e1de615-0768-44a6-8853-9c092dee1588"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[128]: [(&#39;Wed&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7f3ae99e9110&gt;),\n (&#39;Mon&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7f3ae99e9590&gt;),\n (&#39;Tue&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7f3ae99e9990&gt;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[128]: [(&#39;Wed&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7f3ae99e9110&gt;),\n (&#39;Mon&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7f3ae99e9590&gt;),\n (&#39;Tue&#39;, &lt;pyspark.resultiterable.ResultIterable at 0x7f3ae99e9990&gt;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["list_rdd = grouped_RDD.mapValues(lambda x: list(x)) #note that the value will be an iterator\nlist_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b624ade8-cf6e-4c74-96f0-59c85742e467"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[134]: [(&#39;Wed&#39;, [100.0]), (&#39;Mon&#39;, [200.0, 300.25, 100.0]), (&#39;Tue&#39;, [1215.5])]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[134]: [(&#39;Wed&#39;, [100.0]), (&#39;Mon&#39;, [200.0, 300.25, 100.0]), (&#39;Tue&#39;, [1215.5])]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["original_RDD = list_rdd.flatMapValues(lambda x: x)\noriginal_RDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4581569-3287-45f4-878e-2a426e93ad4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[135]: [(&#39;Wed&#39;, 100.0),\n (&#39;Mon&#39;, 200.0),\n (&#39;Mon&#39;, 300.25),\n (&#39;Mon&#39;, 100.0),\n (&#39;Tue&#39;, 1215.5)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[135]: [(&#39;Wed&#39;, 100.0),\n (&#39;Mon&#39;, 200.0),\n (&#39;Mon&#39;, 300.25),\n (&#39;Mon&#39;, 100.0),\n (&#39;Tue&#39;, 1215.5)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sort_by_value = original_RDD.sortBy(lambda item: item[1], ascending = False)  \nsort_by_value.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2a4f632-8a9b-41e0-9317-cf86637b8196"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[137]: [(&#39;Tue&#39;, 1215.5),\n (&#39;Mon&#39;, 300.25),\n (&#39;Mon&#39;, 200.0),\n (&#39;Wed&#39;, 100.0),\n (&#39;Mon&#39;, 100.0)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[137]: [(&#39;Tue&#39;, 1215.5),\n (&#39;Mon&#39;, 300.25),\n (&#39;Mon&#39;, 200.0),\n (&#39;Wed&#39;, 100.0),\n (&#39;Mon&#39;, 100.0)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words = [\"apple\",\"mango\",\"orange\",\"apple\",\"mango\",\"mango\",\"peach\", \"mango\",\"guava\", \"apple\"]\nwords_rdd = sc.parallelize(words)\nwords_rdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50faa331-ec56-4436-a965-ac27821aaa3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[138]: [&#39;apple&#39;, &#39;mango&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;mango&#39;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[138]: [&#39;apple&#39;, &#39;mango&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;mango&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words_rdd.map(lambda word: (word, 1)).countByKey() #note that countByKey() is an action"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c41eb43c-2779-48ea-814b-af24f45370fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[139]: defaultdict(int, {&#39;apple&#39;: 3, &#39;mango&#39;: 4, &#39;orange&#39;: 1, &#39;peach&#39;: 1, &#39;guava&#39;: 1})</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[139]: defaultdict(int, {&#39;apple&#39;: 3, &#39;mango&#39;: 4, &#39;orange&#39;: 1, &#39;peach&#39;: 1, &#39;guava&#39;: 1})</div>"]}}],"execution_count":0},{"cell_type":"code","source":["count_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y) #note that reduceByKey() is a transformation\ncount_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47fc7768-8c13-4084-b7de-3c3f8277076b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[142]: [(&#39;mango&#39;, 4), (&#39;orange&#39;, 1), (&#39;peach&#39;, 1), (&#39;apple&#39;, 3), (&#39;guava&#39;, 1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[142]: [(&#39;mango&#39;, 4), (&#39;orange&#39;, 1), (&#39;peach&#39;, 1), (&#39;apple&#39;, 3), (&#39;guava&#39;, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"408044c1-4e59-424e-8a39-bd8e1b7eadb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"spark_test","dashboards":[],"language":"python","widgets":{},"notebookOrigID":2876317958955845}},"nbformat":4,"nbformat_minor":0}
