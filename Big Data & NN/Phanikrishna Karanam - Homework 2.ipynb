{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall 2020 -> INSY 5376 -> Big Data & Deep Learning\n",
    "## Homework 2\n",
    "### Name: Phanikrishna Karanam (UTA ID# 1001851985)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment on Building a Neural Network from Scratch\n",
    "\n",
    "__Summary__: \n",
    "This program is to build a simple neural network for classification that has the following architecture:\n",
    " - An input layer that can accept any number of X values\n",
    " - A hidden layer (can be specified during model training)  with RELU activation\n",
    " - A single node output with Sigmoid activation \n",
    "\n",
    "__About Dataset__: \n",
    "We will use the breast cancer dataset for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Load breast cancer dataset and examine the dataset shape and target distribution\n",
    " - We notice there is a class imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features:  (569, 30)\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "\n",
      "Shape of Target and count distribution: (569,) [212 357]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "data = load_breast_cancer()\n",
    "features = data.data\n",
    "target = data.target\n",
    "\n",
    "print('Shape of Features: ', features.shape)\n",
    "print(features)\n",
    "print('\\nShape of Target and count distribution: {} {}'.format(target.shape, np.bincount(target)))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Use SMOTE oversampling technique to address the class imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features:  (714, 30)\n",
      "[[1.79900000e+01 1.03800000e+01 1.22800000e+02 ... 2.65400000e-01\n",
      "  4.60100000e-01 1.18900000e-01]\n",
      " [2.05700000e+01 1.77700000e+01 1.32900000e+02 ... 1.86000000e-01\n",
      "  2.75000000e-01 8.90200000e-02]\n",
      " [1.96900000e+01 2.12500000e+01 1.30000000e+02 ... 2.43000000e-01\n",
      "  3.61300000e-01 8.75800000e-02]\n",
      " ...\n",
      " [1.86221425e+01 1.79835121e+01 1.21183671e+02 ... 1.62081919e-01\n",
      "  2.93275889e-01 8.30348826e-02]\n",
      " [2.01986327e+01 1.63098798e+01 1.34241147e+02 ... 1.64455261e-01\n",
      "  2.53595333e-01 7.82181219e-02]\n",
      " [2.13888944e+01 2.05582691e+01 1.44633655e+02 ... 2.72303368e-01\n",
      "  3.59403132e-01 9.01033185e-02]]\n",
      "\n",
      "Shape of Target and count distribution: (714,) [357 357]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversampler = SMOTE(random_state = 32)\n",
    "features, target = oversampler.fit_sample(features, target)\n",
    "\n",
    "print('Shape of Features: ', features.shape)\n",
    "print(features)\n",
    "print('\\nShape of Target and count distribution: {} {}'.format(target.shape, np.bincount(target)))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Input dataset has 30 features and all are of type float with no null values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 714 entries, 0 to 713\n",
      "Data columns (total 30 columns):\n",
      "0     714 non-null float64\n",
      "1     714 non-null float64\n",
      "2     714 non-null float64\n",
      "3     714 non-null float64\n",
      "4     714 non-null float64\n",
      "5     714 non-null float64\n",
      "6     714 non-null float64\n",
      "7     714 non-null float64\n",
      "8     714 non-null float64\n",
      "9     714 non-null float64\n",
      "10    714 non-null float64\n",
      "11    714 non-null float64\n",
      "12    714 non-null float64\n",
      "13    714 non-null float64\n",
      "14    714 non-null float64\n",
      "15    714 non-null float64\n",
      "16    714 non-null float64\n",
      "17    714 non-null float64\n",
      "18    714 non-null float64\n",
      "19    714 non-null float64\n",
      "20    714 non-null float64\n",
      "21    714 non-null float64\n",
      "22    714 non-null float64\n",
      "23    714 non-null float64\n",
      "24    714 non-null float64\n",
      "25    714 non-null float64\n",
      "26    714 non-null float64\n",
      "27    714 non-null float64\n",
      "28    714 non-null float64\n",
      "29    714 non-null float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 167.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(features)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - High variance observed within the data fields, needs scaling of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>14.773998</td>\n",
       "      <td>3.674853</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>12.042500</td>\n",
       "      <td>13.920000</td>\n",
       "      <td>17.409694</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>714.0</td>\n",
       "      <td>19.754546</td>\n",
       "      <td>4.170298</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>19.581675</td>\n",
       "      <td>22.149308</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>714.0</td>\n",
       "      <td>96.551011</td>\n",
       "      <td>25.384176</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>77.587500</td>\n",
       "      <td>90.827448</td>\n",
       "      <td>114.475000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>714.0</td>\n",
       "      <td>716.198075</td>\n",
       "      <td>370.395606</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>445.475000</td>\n",
       "      <td>599.450000</td>\n",
       "      <td>947.950000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.097989</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.087957</td>\n",
       "      <td>0.097894</td>\n",
       "      <td>0.106899</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.114015</td>\n",
       "      <td>0.054624</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.071675</td>\n",
       "      <td>0.107350</td>\n",
       "      <td>0.145398</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.082004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035962</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.155389</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.057052</td>\n",
       "      <td>0.040637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023340</td>\n",
       "      <td>0.050833</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.184393</td>\n",
       "      <td>0.027142</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.164925</td>\n",
       "      <td>0.182850</td>\n",
       "      <td>0.199725</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.063009</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.058215</td>\n",
       "      <td>0.061910</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.442597</td>\n",
       "      <td>0.284508</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.246008</td>\n",
       "      <td>0.358608</td>\n",
       "      <td>0.556228</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>714.0</td>\n",
       "      <td>1.217512</td>\n",
       "      <td>0.533798</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>1.115496</td>\n",
       "      <td>1.452348</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>714.0</td>\n",
       "      <td>3.139632</td>\n",
       "      <td>2.093820</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.775750</td>\n",
       "      <td>2.563500</td>\n",
       "      <td>3.817655</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>714.0</td>\n",
       "      <td>46.030295</td>\n",
       "      <td>46.286468</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>19.161900</td>\n",
       "      <td>29.142330</td>\n",
       "      <td>56.887904</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.027261</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.034374</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.034047</td>\n",
       "      <td>0.028451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017628</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.006118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015664</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>0.018710</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>714.0</td>\n",
       "      <td>17.215982</td>\n",
       "      <td>5.083696</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.332500</td>\n",
       "      <td>15.999853</td>\n",
       "      <td>20.703844</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>714.0</td>\n",
       "      <td>26.418737</td>\n",
       "      <td>6.021542</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.997170</td>\n",
       "      <td>26.266478</td>\n",
       "      <td>30.485000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>714.0</td>\n",
       "      <td>113.928191</td>\n",
       "      <td>35.278015</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>86.677500</td>\n",
       "      <td>105.900000</td>\n",
       "      <td>137.520478</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>714.0</td>\n",
       "      <td>983.753198</td>\n",
       "      <td>604.790857</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>545.375000</td>\n",
       "      <td>779.800000</td>\n",
       "      <td>1306.849632</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.135382</td>\n",
       "      <td>0.022776</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.135640</td>\n",
       "      <td>0.149775</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.282939</td>\n",
       "      <td>0.165827</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.243350</td>\n",
       "      <td>0.375475</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.309556</td>\n",
       "      <td>0.210770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.286121</td>\n",
       "      <td>0.437504</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.128928</td>\n",
       "      <td>0.067812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074130</td>\n",
       "      <td>0.125367</td>\n",
       "      <td>0.180746</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.298888</td>\n",
       "      <td>0.063706</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.256475</td>\n",
       "      <td>0.291550</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.086067</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.073225</td>\n",
       "      <td>0.081820</td>\n",
       "      <td>0.095147</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count        mean         std         min         25%         50%  \\\n",
       "0   714.0   14.773998    3.674853    6.981000   12.042500   13.920000   \n",
       "1   714.0   19.754546    4.170298    9.710000   16.850000   19.581675   \n",
       "2   714.0   96.551011   25.384176   43.790000   77.587500   90.827448   \n",
       "3   714.0  716.198075  370.395606  143.500000  445.475000  599.450000   \n",
       "4   714.0    0.097989    0.013791    0.052630    0.087957    0.097894   \n",
       "5   714.0    0.114015    0.054624    0.019380    0.071675    0.107350   \n",
       "6   714.0    0.103858    0.082004    0.000000    0.035962    0.088210   \n",
       "7   714.0    0.057052    0.040637    0.000000    0.023340    0.050833   \n",
       "8   714.0    0.184393    0.027142    0.106000    0.164925    0.182850   \n",
       "9   714.0    0.063009    0.006912    0.049960    0.058215    0.061910   \n",
       "10  714.0    0.442597    0.284508    0.111500    0.246008    0.358608   \n",
       "11  714.0    1.217512    0.533798    0.360200    0.873400    1.115496   \n",
       "12  714.0    3.139632    2.093820    0.757000    1.775750    2.563500   \n",
       "13  714.0   46.030295   46.286468    6.802000   19.161900   29.142330   \n",
       "14  714.0    0.007000    0.002858    0.001713    0.005238    0.006382   \n",
       "15  714.0    0.027261    0.018094    0.002252    0.014270    0.023100   \n",
       "16  714.0    0.034047    0.028451    0.000000    0.017628    0.029255   \n",
       "17  714.0    0.012528    0.006118    0.000000    0.008584    0.011845   \n",
       "18  714.0    0.020696    0.008211    0.007882    0.015263    0.018710   \n",
       "19  714.0    0.003904    0.002505    0.000895    0.002363    0.003394   \n",
       "20  714.0   17.215982    5.083696    7.930000   13.332500   15.999853   \n",
       "21  714.0   26.418737    6.021542   12.020000   21.997170   26.266478   \n",
       "22  714.0  113.928191   35.278015   50.410000   86.677500  105.900000   \n",
       "23  714.0  983.753198  604.790857  185.200000  545.375000  779.800000   \n",
       "24  714.0    0.135382    0.022776    0.071170    0.120700    0.135640   \n",
       "25  714.0    0.282939    0.165827    0.027290    0.162900    0.243350   \n",
       "26  714.0    0.309556    0.210770    0.000000    0.139550    0.286121   \n",
       "27  714.0    0.128928    0.067812    0.000000    0.074130    0.125367   \n",
       "28  714.0    0.298888    0.063706    0.156500    0.256475    0.291550   \n",
       "29  714.0    0.086067    0.018372    0.055040    0.073225    0.081820   \n",
       "\n",
       "            75%         max  \n",
       "0     17.409694    28.11000  \n",
       "1     22.149308    39.28000  \n",
       "2    114.475000   188.50000  \n",
       "3    947.950000  2501.00000  \n",
       "4      0.106899     0.16340  \n",
       "5      0.145398     0.34540  \n",
       "6      0.155389     0.42680  \n",
       "7      0.086300     0.20120  \n",
       "8      0.199725     0.30400  \n",
       "9      0.066421     0.09744  \n",
       "10     0.556228     2.87300  \n",
       "11     1.452348     4.88500  \n",
       "12     3.817655    21.98000  \n",
       "13    56.887904   542.20000  \n",
       "14     0.008107     0.03113  \n",
       "15     0.034374     0.13540  \n",
       "16     0.044299     0.39600  \n",
       "17     0.015664     0.05279  \n",
       "18     0.023830     0.07895  \n",
       "19     0.004620     0.02984  \n",
       "20    20.703844    36.04000  \n",
       "21    30.485000    49.54000  \n",
       "22   137.520478   251.20000  \n",
       "23  1306.849632  4254.00000  \n",
       "24     0.149775     0.22260  \n",
       "25     0.375475     1.05800  \n",
       "26     0.437504     1.25200  \n",
       "27     0.180746     0.29100  \n",
       "28     0.325800     0.66380  \n",
       "29     0.095147     0.20750  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Split the input data into train and test, by performing a stratified split on the target variable (to ensure equal target      distribution) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (571, 30)\n",
      "Shape of y_train and count distribution: (571,) [286 285]\n",
      "\n",
      "Shape of x_test: (143, 30)\n",
      "Shape of y_test and count distribution: (143,) [71 72]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 32, stratify = target)\n",
    "print('Shape of x_train:', x_train.shape)\n",
    "print('Shape of y_train and count distribution: {} {}'.format(y_train.shape, np.bincount(y_train)))\n",
    "\n",
    "print('\\nShape of x_test:', x_test.shape)\n",
    "print('Shape of y_test and count distribution: {} {}'.format(y_test.shape, np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Performing scaling on the input data\n",
    "   - Use train data to fit and transform both the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Notice the unit standard deviation on train data after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.386882e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.122983</td>\n",
       "      <td>-0.755809</td>\n",
       "      <td>-0.232397</td>\n",
       "      <td>0.736564</td>\n",
       "      <td>3.423265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>571.0</td>\n",
       "      <td>8.677142e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.421295</td>\n",
       "      <td>-0.702734</td>\n",
       "      <td>-0.023023</td>\n",
       "      <td>0.585583</td>\n",
       "      <td>4.706031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-1.352100e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.084622</td>\n",
       "      <td>-0.760128</td>\n",
       "      <td>-0.201807</td>\n",
       "      <td>0.729787</td>\n",
       "      <td>3.417429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>571.0</td>\n",
       "      <td>1.727359e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.570863</td>\n",
       "      <td>-0.744594</td>\n",
       "      <td>-0.313328</td>\n",
       "      <td>0.661033</td>\n",
       "      <td>4.260078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-5.875821e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-3.254062</td>\n",
       "      <td>-0.718756</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>0.631329</td>\n",
       "      <td>4.715739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-3.153733e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.730448</td>\n",
       "      <td>-0.766067</td>\n",
       "      <td>-0.115404</td>\n",
       "      <td>0.542114</td>\n",
       "      <td>4.278870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-2.391549e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.262357</td>\n",
       "      <td>-0.808843</td>\n",
       "      <td>-0.187543</td>\n",
       "      <td>0.631248</td>\n",
       "      <td>4.011030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>571.0</td>\n",
       "      <td>1.856464e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.399556</td>\n",
       "      <td>-0.815599</td>\n",
       "      <td>-0.169097</td>\n",
       "      <td>0.731935</td>\n",
       "      <td>3.613977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-4.388395e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.908561</td>\n",
       "      <td>-0.712773</td>\n",
       "      <td>-0.085139</td>\n",
       "      <td>0.542931</td>\n",
       "      <td>4.466606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>571.0</td>\n",
       "      <td>1.025119e-14</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.859821</td>\n",
       "      <td>-0.683748</td>\n",
       "      <td>-0.185543</td>\n",
       "      <td>0.516006</td>\n",
       "      <td>4.900467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>571.0</td>\n",
       "      <td>8.496803e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.249787</td>\n",
       "      <td>-0.737200</td>\n",
       "      <td>-0.297923</td>\n",
       "      <td>0.459165</td>\n",
       "      <td>4.911623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-2.350231e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.595627</td>\n",
       "      <td>-0.620995</td>\n",
       "      <td>-0.188818</td>\n",
       "      <td>0.411671</td>\n",
       "      <td>6.784469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-5.696941e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.211316</td>\n",
       "      <td>-0.699671</td>\n",
       "      <td>-0.279805</td>\n",
       "      <td>0.375449</td>\n",
       "      <td>4.440160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-4.155073e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-0.978984</td>\n",
       "      <td>-0.660825</td>\n",
       "      <td>-0.409048</td>\n",
       "      <td>0.341521</td>\n",
       "      <td>5.973824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.536524e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.857168</td>\n",
       "      <td>-0.617067</td>\n",
       "      <td>-0.202755</td>\n",
       "      <td>0.397546</td>\n",
       "      <td>8.564221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>571.0</td>\n",
       "      <td>8.487081e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.352135</td>\n",
       "      <td>-0.718592</td>\n",
       "      <td>-0.225872</td>\n",
       "      <td>0.373318</td>\n",
       "      <td>5.865099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.922356e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.137803</td>\n",
       "      <td>-0.547826</td>\n",
       "      <td>-0.175666</td>\n",
       "      <td>0.329356</td>\n",
       "      <td>12.091571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-6.089700e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.004587</td>\n",
       "      <td>-0.638113</td>\n",
       "      <td>-0.107902</td>\n",
       "      <td>0.502148</td>\n",
       "      <td>6.459151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.411478e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.535962</td>\n",
       "      <td>-0.646980</td>\n",
       "      <td>-0.236083</td>\n",
       "      <td>0.336232</td>\n",
       "      <td>6.964204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.956382e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.187537</td>\n",
       "      <td>-0.623410</td>\n",
       "      <td>-0.203502</td>\n",
       "      <td>0.273291</td>\n",
       "      <td>10.195681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-3.661014e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.821962</td>\n",
       "      <td>-0.761635</td>\n",
       "      <td>-0.243801</td>\n",
       "      <td>0.719223</td>\n",
       "      <td>3.206114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-5.183876e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.431388</td>\n",
       "      <td>-0.746421</td>\n",
       "      <td>-0.013460</td>\n",
       "      <td>0.682431</td>\n",
       "      <td>3.890609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-3.051655e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.800106</td>\n",
       "      <td>-0.767416</td>\n",
       "      <td>-0.219976</td>\n",
       "      <td>0.683211</td>\n",
       "      <td>3.365039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>571.0</td>\n",
       "      <td>6.517456e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.337005</td>\n",
       "      <td>-0.733431</td>\n",
       "      <td>-0.354967</td>\n",
       "      <td>0.566980</td>\n",
       "      <td>3.980624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>571.0</td>\n",
       "      <td>5.726009e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.801906</td>\n",
       "      <td>-0.671965</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.628705</td>\n",
       "      <td>3.816953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-6.227748e-16</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.566950</td>\n",
       "      <td>-0.731650</td>\n",
       "      <td>-0.232605</td>\n",
       "      <td>0.578030</td>\n",
       "      <td>4.050304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>571.0</td>\n",
       "      <td>1.558590e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.484022</td>\n",
       "      <td>-0.816256</td>\n",
       "      <td>-0.109814</td>\n",
       "      <td>0.607978</td>\n",
       "      <td>4.578652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.255444e-17</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.888797</td>\n",
       "      <td>-0.800323</td>\n",
       "      <td>-0.044076</td>\n",
       "      <td>0.747754</td>\n",
       "      <td>2.402280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>571.0</td>\n",
       "      <td>-4.716358e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-2.240552</td>\n",
       "      <td>-0.663930</td>\n",
       "      <td>-0.112402</td>\n",
       "      <td>0.426792</td>\n",
       "      <td>5.700434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2.535819e-15</td>\n",
       "      <td>1.000877</td>\n",
       "      <td>-1.733705</td>\n",
       "      <td>-0.730651</td>\n",
       "      <td>-0.238640</td>\n",
       "      <td>0.527807</td>\n",
       "      <td>4.868986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count          mean       std       min       25%       50%       75%  \\\n",
       "0   571.0  2.386882e-15  1.000877 -2.122983 -0.755809 -0.232397  0.736564   \n",
       "1   571.0  8.677142e-16  1.000877 -2.421295 -0.702734 -0.023023  0.585583   \n",
       "2   571.0 -1.352100e-15  1.000877 -2.084622 -0.760128 -0.201807  0.729787   \n",
       "3   571.0  1.727359e-15  1.000877 -1.570863 -0.744594 -0.313328  0.661033   \n",
       "4   571.0 -5.875821e-16  1.000877 -3.254062 -0.718756 -0.001963  0.631329   \n",
       "5   571.0 -3.153733e-16  1.000877 -1.730448 -0.766067 -0.115404  0.542114   \n",
       "6   571.0 -2.391549e-16  1.000877 -1.262357 -0.808843 -0.187543  0.631248   \n",
       "7   571.0  1.856464e-15  1.000877 -1.399556 -0.815599 -0.169097  0.731935   \n",
       "8   571.0 -4.388395e-15  1.000877 -2.908561 -0.712773 -0.085139  0.542931   \n",
       "9   571.0  1.025119e-14  1.000877 -1.859821 -0.683748 -0.185543  0.516006   \n",
       "10  571.0  8.496803e-16  1.000877 -1.249787 -0.737200 -0.297923  0.459165   \n",
       "11  571.0 -2.350231e-16  1.000877 -1.595627 -0.620995 -0.188818  0.411671   \n",
       "12  571.0 -5.696941e-16  1.000877 -1.211316 -0.699671 -0.279805  0.375449   \n",
       "13  571.0 -4.155073e-16  1.000877 -0.978984 -0.660825 -0.409048  0.341521   \n",
       "14  571.0  2.536524e-15  1.000877 -1.857168 -0.617067 -0.202755  0.397546   \n",
       "15  571.0  8.487081e-16  1.000877 -1.352135 -0.718592 -0.225872  0.373318   \n",
       "16  571.0  2.922356e-16  1.000877 -1.137803 -0.547826 -0.175666  0.329356   \n",
       "17  571.0 -6.089700e-16  1.000877 -2.004587 -0.638113 -0.107902  0.502148   \n",
       "18  571.0  2.411478e-16  1.000877 -1.535962 -0.646980 -0.236083  0.336232   \n",
       "19  571.0  2.956382e-16  1.000877 -1.187537 -0.623410 -0.203502  0.273291   \n",
       "20  571.0 -3.661014e-15  1.000877 -1.821962 -0.761635 -0.243801  0.719223   \n",
       "21  571.0 -5.183876e-16  1.000877 -2.431388 -0.746421 -0.013460  0.682431   \n",
       "22  571.0 -3.051655e-15  1.000877 -1.800106 -0.767416 -0.219976  0.683211   \n",
       "23  571.0  6.517456e-16  1.000877 -1.337005 -0.733431 -0.354967  0.566980   \n",
       "24  571.0  5.726009e-15  1.000877 -2.801906 -0.671965  0.023008  0.628705   \n",
       "25  571.0 -6.227748e-16  1.000877 -1.566950 -0.731650 -0.232605  0.578030   \n",
       "26  571.0  1.558590e-15  1.000877 -1.484022 -0.816256 -0.109814  0.607978   \n",
       "27  571.0  2.255444e-17  1.000877 -1.888797 -0.800323 -0.044076  0.747754   \n",
       "28  571.0 -4.716358e-15  1.000877 -2.240552 -0.663930 -0.112402  0.426792   \n",
       "29  571.0  2.535819e-15  1.000877 -1.733705 -0.730651 -0.238640  0.527807   \n",
       "\n",
       "          max  \n",
       "0    3.423265  \n",
       "1    4.706031  \n",
       "2    3.417429  \n",
       "3    4.260078  \n",
       "4    4.715739  \n",
       "5    4.278870  \n",
       "6    4.011030  \n",
       "7    3.613977  \n",
       "8    4.466606  \n",
       "9    4.900467  \n",
       "10   4.911623  \n",
       "11   6.784469  \n",
       "12   4.440160  \n",
       "13   5.973824  \n",
       "14   8.564221  \n",
       "15   5.865099  \n",
       "16  12.091571  \n",
       "17   6.459151  \n",
       "18   6.964204  \n",
       "19  10.195681  \n",
       "20   3.206114  \n",
       "21   3.890609  \n",
       "22   3.365039  \n",
       "23   3.980624  \n",
       "24   3.816953  \n",
       "25   4.050304  \n",
       "26   4.578652  \n",
       "27   2.402280  \n",
       "28   5.700434  \n",
       "29   4.868986  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x_train)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Since data is fit on training and then the same stats are applied for transforming test data, the std dev will not be of unit variance like in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.062872</td>\n",
       "      <td>1.033366</td>\n",
       "      <td>-1.603133</td>\n",
       "      <td>-0.666747</td>\n",
       "      <td>-0.215955</td>\n",
       "      <td>0.691110</td>\n",
       "      <td>3.667159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>1.025728</td>\n",
       "      <td>-2.042874</td>\n",
       "      <td>-0.670195</td>\n",
       "      <td>-0.098338</td>\n",
       "      <td>0.515684</td>\n",
       "      <td>2.739207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.071108</td>\n",
       "      <td>1.046324</td>\n",
       "      <td>-1.569463</td>\n",
       "      <td>-0.691904</td>\n",
       "      <td>-0.234030</td>\n",
       "      <td>0.692460</td>\n",
       "      <td>3.672025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.071929</td>\n",
       "      <td>1.119341</td>\n",
       "      <td>-1.300976</td>\n",
       "      <td>-0.663766</td>\n",
       "      <td>-0.306250</td>\n",
       "      <td>0.602903</td>\n",
       "      <td>4.954864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.047303</td>\n",
       "      <td>0.959383</td>\n",
       "      <td>-2.128058</td>\n",
       "      <td>-0.637633</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.736955</td>\n",
       "      <td>2.199174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.069442</td>\n",
       "      <td>1.032056</td>\n",
       "      <td>-1.495619</td>\n",
       "      <td>-0.770767</td>\n",
       "      <td>-0.014026</td>\n",
       "      <td>0.740779</td>\n",
       "      <td>3.120461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.104222</td>\n",
       "      <td>1.060544</td>\n",
       "      <td>-1.262357</td>\n",
       "      <td>-0.828550</td>\n",
       "      <td>-0.116495</td>\n",
       "      <td>0.858341</td>\n",
       "      <td>3.228918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.110260</td>\n",
       "      <td>1.057101</td>\n",
       "      <td>-1.399556</td>\n",
       "      <td>-0.819835</td>\n",
       "      <td>-0.044257</td>\n",
       "      <td>0.796559</td>\n",
       "      <td>3.367288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.057229</td>\n",
       "      <td>1.052764</td>\n",
       "      <td>-2.331212</td>\n",
       "      <td>-0.692286</td>\n",
       "      <td>0.123776</td>\n",
       "      <td>0.750122</td>\n",
       "      <td>3.118217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.009309</td>\n",
       "      <td>0.917261</td>\n",
       "      <td>-1.791478</td>\n",
       "      <td>-0.695850</td>\n",
       "      <td>-0.105677</td>\n",
       "      <td>0.436702</td>\n",
       "      <td>4.659842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.138368</td>\n",
       "      <td>1.419195</td>\n",
       "      <td>-1.235125</td>\n",
       "      <td>-0.696880</td>\n",
       "      <td>-0.272747</td>\n",
       "      <td>0.511356</td>\n",
       "      <td>9.405150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.039216</td>\n",
       "      <td>0.940785</td>\n",
       "      <td>-1.590811</td>\n",
       "      <td>-0.708907</td>\n",
       "      <td>-0.260679</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>3.158181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.130923</td>\n",
       "      <td>1.405736</td>\n",
       "      <td>-1.218841</td>\n",
       "      <td>-0.624170</td>\n",
       "      <td>-0.233820</td>\n",
       "      <td>0.389591</td>\n",
       "      <td>9.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>1.772692</td>\n",
       "      <td>-0.922011</td>\n",
       "      <td>-0.648029</td>\n",
       "      <td>-0.349335</td>\n",
       "      <td>0.174848</td>\n",
       "      <td>12.860932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.079569</td>\n",
       "      <td>1.059546</td>\n",
       "      <td>-1.458621</td>\n",
       "      <td>-0.558436</td>\n",
       "      <td>-0.207714</td>\n",
       "      <td>0.451087</td>\n",
       "      <td>5.248308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>0.899186</td>\n",
       "      <td>-1.310939</td>\n",
       "      <td>-0.666013</td>\n",
       "      <td>-0.188651</td>\n",
       "      <td>0.495320</td>\n",
       "      <td>4.293166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.001840</td>\n",
       "      <td>0.717636</td>\n",
       "      <td>-1.137803</td>\n",
       "      <td>-0.546657</td>\n",
       "      <td>-0.109185</td>\n",
       "      <td>0.370212</td>\n",
       "      <td>2.189585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.020360</td>\n",
       "      <td>0.899606</td>\n",
       "      <td>-2.004587</td>\n",
       "      <td>-0.570058</td>\n",
       "      <td>-0.076294</td>\n",
       "      <td>0.507422</td>\n",
       "      <td>3.586066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.016602</td>\n",
       "      <td>0.906505</td>\n",
       "      <td>-1.337775</td>\n",
       "      <td>-0.659489</td>\n",
       "      <td>-0.283925</td>\n",
       "      <td>0.476941</td>\n",
       "      <td>4.252734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.020180</td>\n",
       "      <td>0.923150</td>\n",
       "      <td>-1.165749</td>\n",
       "      <td>-0.598539</td>\n",
       "      <td>-0.208221</td>\n",
       "      <td>0.287604</td>\n",
       "      <td>7.450671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.049357</td>\n",
       "      <td>1.013320</td>\n",
       "      <td>-1.529213</td>\n",
       "      <td>-0.716263</td>\n",
       "      <td>-0.164893</td>\n",
       "      <td>0.667714</td>\n",
       "      <td>3.723303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.026204</td>\n",
       "      <td>1.071091</td>\n",
       "      <td>-2.064065</td>\n",
       "      <td>-0.780120</td>\n",
       "      <td>-0.077489</td>\n",
       "      <td>0.658895</td>\n",
       "      <td>3.489587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.059414</td>\n",
       "      <td>1.030298</td>\n",
       "      <td>-1.431247</td>\n",
       "      <td>-0.728904</td>\n",
       "      <td>-0.140100</td>\n",
       "      <td>0.682914</td>\n",
       "      <td>3.927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.055566</td>\n",
       "      <td>1.100374</td>\n",
       "      <td>-1.193844</td>\n",
       "      <td>-0.709072</td>\n",
       "      <td>-0.286489</td>\n",
       "      <td>0.538207</td>\n",
       "      <td>5.532026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.023756</td>\n",
       "      <td>0.976969</td>\n",
       "      <td>-2.237186</td>\n",
       "      <td>-0.580176</td>\n",
       "      <td>-0.025072</td>\n",
       "      <td>0.653409</td>\n",
       "      <td>3.587906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.050253</td>\n",
       "      <td>1.109467</td>\n",
       "      <td>-1.444625</td>\n",
       "      <td>-0.725790</td>\n",
       "      <td>-0.251727</td>\n",
       "      <td>0.631937</td>\n",
       "      <td>4.791161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.074732</td>\n",
       "      <td>1.097643</td>\n",
       "      <td>-1.484022</td>\n",
       "      <td>-0.771464</td>\n",
       "      <td>-0.049706</td>\n",
       "      <td>0.712079</td>\n",
       "      <td>4.181576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.061773</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>-1.888797</td>\n",
       "      <td>-0.720179</td>\n",
       "      <td>0.045874</td>\n",
       "      <td>0.855428</td>\n",
       "      <td>2.385803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>143.0</td>\n",
       "      <td>-0.058374</td>\n",
       "      <td>0.984555</td>\n",
       "      <td>-2.110629</td>\n",
       "      <td>-0.693211</td>\n",
       "      <td>-0.161857</td>\n",
       "      <td>0.368875</td>\n",
       "      <td>4.347977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.014873</td>\n",
       "      <td>1.135159</td>\n",
       "      <td>-1.721950</td>\n",
       "      <td>-0.671039</td>\n",
       "      <td>-0.214571</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>6.800088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count      mean       std       min       25%       50%       75%  \\\n",
       "0   143.0  0.062872  1.033366 -1.603133 -0.666747 -0.215955  0.691110   \n",
       "1   143.0 -0.001171  1.025728 -2.042874 -0.670195 -0.098338  0.515684   \n",
       "2   143.0  0.071108  1.046324 -1.569463 -0.691904 -0.234030  0.692460   \n",
       "3   143.0  0.071929  1.119341 -1.300976 -0.663766 -0.306250  0.602903   \n",
       "4   143.0  0.047303  0.959383 -2.128058 -0.637633  0.008110  0.736955   \n",
       "5   143.0  0.069442  1.032056 -1.495619 -0.770767 -0.014026  0.740779   \n",
       "6   143.0  0.104222  1.060544 -1.262357 -0.828550 -0.116495  0.858341   \n",
       "7   143.0  0.110260  1.057101 -1.399556 -0.819835 -0.044257  0.796559   \n",
       "8   143.0  0.057229  1.052764 -2.331212 -0.692286  0.123776  0.750122   \n",
       "9   143.0 -0.009309  0.917261 -1.791478 -0.695850 -0.105677  0.436702   \n",
       "10  143.0  0.138368  1.419195 -1.235125 -0.696880 -0.272747  0.511356   \n",
       "11  143.0 -0.039216  0.940785 -1.590811 -0.708907 -0.260679  0.482914   \n",
       "12  143.0  0.130923  1.405736 -1.218841 -0.624170 -0.233820  0.389591   \n",
       "13  143.0  0.175047  1.772692 -0.922011 -0.648029 -0.349335  0.174848   \n",
       "14  143.0  0.079569  1.059546 -1.458621 -0.558436 -0.207714  0.451087   \n",
       "15  143.0  0.017274  0.899186 -1.310939 -0.666013 -0.188651  0.495320   \n",
       "16  143.0 -0.001840  0.717636 -1.137803 -0.546657 -0.109185  0.370212   \n",
       "17  143.0  0.020360  0.899606 -2.004587 -0.570058 -0.076294  0.507422   \n",
       "18  143.0 -0.016602  0.906505 -1.337775 -0.659489 -0.283925  0.476941   \n",
       "19  143.0 -0.020180  0.923150 -1.165749 -0.598539 -0.208221  0.287604   \n",
       "20  143.0  0.049357  1.013320 -1.529213 -0.716263 -0.164893  0.667714   \n",
       "21  143.0 -0.026204  1.071091 -2.064065 -0.780120 -0.077489  0.658895   \n",
       "22  143.0  0.059414  1.030298 -1.431247 -0.728904 -0.140100  0.682914   \n",
       "23  143.0  0.055566  1.100374 -1.193844 -0.709072 -0.286489  0.538207   \n",
       "24  143.0  0.023756  0.976969 -2.237186 -0.580176 -0.025072  0.653409   \n",
       "25  143.0  0.050253  1.109467 -1.444625 -0.725790 -0.251727  0.631937   \n",
       "26  143.0  0.074732  1.097643 -1.484022 -0.771464 -0.049706  0.712079   \n",
       "27  143.0  0.061773  0.998265 -1.888797 -0.720179  0.045874  0.855428   \n",
       "28  143.0 -0.058374  0.984555 -2.110629 -0.693211 -0.161857  0.368875   \n",
       "29  143.0  0.014873  1.135159 -1.721950 -0.671039 -0.214571  0.400120   \n",
       "\n",
       "          max  \n",
       "0    3.667159  \n",
       "1    2.739207  \n",
       "2    3.672025  \n",
       "3    4.954864  \n",
       "4    2.199174  \n",
       "5    3.120461  \n",
       "6    3.228918  \n",
       "7    3.367288  \n",
       "8    3.118217  \n",
       "9    4.659842  \n",
       "10   9.405150  \n",
       "11   3.158181  \n",
       "12   9.871400  \n",
       "13  12.860932  \n",
       "14   5.248308  \n",
       "15   4.293166  \n",
       "16   2.189585  \n",
       "17   3.586066  \n",
       "18   4.252734  \n",
       "19   7.450671  \n",
       "20   3.723303  \n",
       "21   3.489587  \n",
       "22   3.927900  \n",
       "23   5.532026  \n",
       "24   3.587906  \n",
       "25   4.791161  \n",
       "26   4.181576  \n",
       "27   2.385803  \n",
       "28   4.347977  \n",
       "29   6.800088  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x_test)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Define all the necessary functions required to implement a Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# RELU activation function at the hidden layer\n",
    "def Relu(x):\n",
    "    return (np.maximum(0, x))\n",
    "\n",
    "# Sigmoid activation function at the output layer. Will return a probability value for the transformed output\n",
    "def Sigmoid(z):\n",
    "    return (1 / (1 + np.exp(-z)))\n",
    "\n",
    "# Compute the loss using binary cross entropy function, for every batch of records being processed and return the loss\n",
    "def compute_loss(y, a):\n",
    "    loss = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        loss += -(y[i] * np.log(a[i])) - ((1 - y[i]) * np.log(1 - a[i]))\n",
    "    loss_per_batch = loss / a.shape[0]\n",
    "    return (loss_per_batch)\n",
    "\n",
    "# Compute the deravative of RELU function, to be used in gradient calculations. Will return 1, when input is >= 0\n",
    "def derivative_relu(x):\n",
    "    return (x >= 0)\n",
    "\n",
    "# Train the model using training data. Accepts # of hidden nodes, batch size, learning rate and # of epochs as parms\n",
    "def train_model(x_train, y_train, number_hidden_nodes = 512, batch_size = 32, learning_rate = 0.001, epochs = 50):\n",
    "    number_output_nodes = 1 \n",
    "    losses_epoch = []                  # to store the loss per epoch\n",
    "    \n",
    "    # Assign random weights for:\n",
    "    # a) weights between input to hidden -> size is # of input features * # of hidden nodes\n",
    "    # b) weights between hidden to ouput -> size is # of input features * # of output nodes\n",
    "    weights_input_hidden = 0.02 * np.random.random((x_train.shape[1], number_hidden_nodes)) - 0.01\n",
    "    weights_hidden_output = 0.02 * np.random.random((number_hidden_nodes, number_output_nodes)) - 0.01\n",
    "    \n",
    "    # Determine # of iterations required to complete one epoch, based on batch size specified\n",
    "    iterations_per_batch = math.ceil(x_train.shape[0] / batch_size)\n",
    "    numbers = range(x_train.shape[0])\n",
    "\n",
    "    for epoch in range(epochs):                 # loop thru the specified # of epochs\n",
    "        loss = 0\n",
    "        for i in range(iterations_per_batch):   # Per epoch, loop thru # of iterations, based on batch size\n",
    "            # Pick random samples from data equivalent to batch size \n",
    "            random_samples_index = np.random.choice(numbers, batch_size, replace = False)\n",
    "            x_train_batch = x_train[random_samples_index, : ]\n",
    "            y_train_batch = y_train[random_samples_index]\n",
    "            \n",
    "            # Feed Forward pass \n",
    "            input_to_hidden = x_train_batch.dot(weights_input_hidden)\n",
    "            output_from_hidden = Relu(input_to_hidden)\n",
    "             \n",
    "            hidden_to_output = output_from_hidden.dot(weights_hidden_output)\n",
    "            a = Sigmoid(hidden_to_output)       # predicted output\n",
    "                \n",
    "            # Back propagation: \n",
    "            # a) Determine loss  \n",
    "            loss += compute_loss(y_train_batch, a)\n",
    "            \n",
    "            # b) Compute error gradient for weights_hidden_output & weights_input_hidden\n",
    "            # delta w1 =  (a - y) . output_from_hidden\n",
    "            dz = a - y_train_batch.reshape(a.shape)\n",
    "            dw_hidden_output = output_from_hidden.T.dot(dz)\n",
    "            \n",
    "            # delta w0 =  (a - y) . (weights_hidden_output) . (1 when x > 0) . Xi\n",
    "            dz_hidden = dz.dot(weights_hidden_output.T) \n",
    "            da_hidden = derivative_relu(dz_hidden)\n",
    "            dw_input_hidden = x_train_batch.T.dot(da_hidden)\n",
    "            \n",
    "            # c) Adjust the weights based on the error gradients determined above and learning rate  \n",
    "            weights_input_hidden -= (learning_rate * dw_input_hidden)\n",
    "            weights_hidden_output -= (learning_rate * dw_hidden_output) \n",
    "            \n",
    "        avg_loss_epoch = loss / iterations_per_batch     # determine average loss per epoch and collect for all epochs\n",
    "        losses_epoch.append(avg_loss_epoch) \n",
    "        if epoch % 5 == 0:                               # for every 5 epochs, print the average loss\n",
    "            print('Epoch # {}, Loss: {}'.format(epoch, avg_loss_epoch))\n",
    "    return (losses_epoch, weights_input_hidden, weights_hidden_output)  # return the calibrated weights aling with losses\n",
    "\n",
    "# Function to predict on the test data using the model trained with training data. \n",
    "# Run a feed forward pass to determine the output probabilities. Based on the probability threshold, convert to 1 or 0\n",
    "def predict_target(x_test, weights_ih, weights_ho):\n",
    "    zh = x_test.dot(weights_ih)\n",
    "    ah = Relu(zh)\n",
    "    z = ah.dot(weights_ho)\n",
    "    a = Sigmoid(z)\n",
    "    y_pred = np.where(a >= 0.5, 1, 0)\n",
    "    return (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__: \n",
    "- Update # of hidden nodes, batch size and learning rate -> to view the impact on overall accuracy  \n",
    "\n",
    "__Note__: \n",
    "- Changing the learning rate signficantly impacts the training and test accuracy (between 92% to 99%) and loss / epoch\n",
    "- Tried changing the hidden nodes and batch size, not much of an impact when compared to learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0, Loss: [0.26800889]\n",
      "Epoch # 5, Loss: [nan]\n",
      "Epoch # 10, Loss: [nan]\n",
      "Epoch # 15, Loss: [nan]\n",
      "Epoch # 20, Loss: [nan]\n",
      "Epoch # 25, Loss: [nan]\n",
      "Epoch # 30, Loss: [nan]\n",
      "Epoch # 35, Loss: [nan]\n",
      "Epoch # 40, Loss: [nan]\n",
      "Epoch # 45, Loss: [nan]\n",
      "Epoch # 50, Loss: [nan]\n",
      "Epoch # 55, Loss: [nan]\n",
      "Epoch # 60, Loss: [nan]\n",
      "Epoch # 65, Loss: [nan]\n",
      "Epoch # 70, Loss: [nan]\n",
      "Epoch # 75, Loss: [nan]\n",
      "Epoch # 80, Loss: [nan]\n",
      "Epoch # 85, Loss: [nan]\n",
      "Epoch # 90, Loss: [nan]\n",
      "Epoch # 95, Loss: [nan]\n"
     ]
    }
   ],
   "source": [
    "losses_train, weights_ih, weights_ho = \\\n",
    "        train_model(x_train, y_train, number_hidden_nodes = 512, batch_size = 32, learning_rate = 0.001, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predict on Training and Test data using the weights obtained from model training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = predict_target(x_train, weights_ih, weights_ho)\n",
    "y_pred_test = predict_target(x_test, weights_ih, weights_ho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Print the Classification Report for both Training and Test data\n",
    "   - Accuracy and F1 score for both training and test data look consistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Training:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       286\n",
      "           1       0.96      0.99      0.97       285\n",
      "\n",
      "    accuracy                           0.97       571\n",
      "   macro avg       0.97      0.97      0.97       571\n",
      "weighted avg       0.97      0.97      0.97       571\n",
      "\n",
      "Classification report for Test:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97        71\n",
      "           1       0.96      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.97      0.97      0.97       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Classification report for Training:\\n')\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "print('Classification report for Test:\\n')\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - As expected, we see a downward trend in the loss for rising epochs (no loss available after 1 epoch)\n",
    "   - When learning rate is increased, we do see a change in the loss / epoch and the model tends to overfit after about 40+ epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvYklEQVR4nO3dd3hUdfr+8feThNA7iEhvgihNQ5GSWOgW7KIs9oKKtF3bruu6P93iFooIKtgLYkNFRZqrhA5BepXepXekyPP7Yw67+eIAQ8hkUu7Xdc3lnM85Z/KcCc6dU+Y55u6IiIicKC7WBYiISPakgBARkbAUECIiEpYCQkREwlJAiIhIWAoIEREJSwEhUWdm35jZnZm9rOQOZva9md0X6zrk1xQQEpaZ7Uv3OGZmB9NNdzmT13L3Du7+dmYvm9nMzM2sZix+djSY2Xlmtv4M13nLzA6f8PufG60aJXtLiHUBkj25e5Hjz81sNXCfu48/cTkzS3D3o1lZm0SsIzA6A+v9w92fzuxiJOfRHoScETO7zMzWm9kTZrYZeNPMSprZV2a21cx2Bs8rplvnv4cQzOwuM5tkZv8Kll1lZh0yuGw1M0s1s71mNt7MBpnZe1HY5uJm9k6wfWvM7Gkziwvm1TSzCWa228y2mdmHwbiZWT8z2xLMm2dmFwXz8gfbtNbMfjKzV8ysYDCvTPD+7TKzHWY28fjPOqGmV8zsXyeMfWFmfdINdQRGBfOeMLMNwXu11MyuzMD7UDXYy3rAzDaa2SYz+226+fnNrH8wb2PwPH+6+Z3MbI6Z7TGzFWbWPt3LVzGzyUF9Y82sTLBOATN7z8y2B+/JTDMrd6a1S8YoICQjzgVKAVWABwj9O3ozmK4MHAReOsX6TYGlQBngH8DrZmYZWHYYMAMoDTwLdM3wFp3aQKA4UB1IAe4A7g7mPQeMBUoCFYNlAdoCycD5QAngVmB7MO+FYLwhUBOoADwTzPstsB4oC5QDfg+E64czDLj1+HthZiWDnzk8mM4X/PxxZlYb6A40dveiQDtgdYbeiZDLgVrBz3vSzFoH438AmgXb1QBoAjwd1NMEeAd4jND7kXxCDbcTek/PARKB3wXjdxJ67ysR+j13I/TvS7KCu+uhxykfhP5Hbh08vww4DBQ4xfINgZ3ppr8ndIgK4C5gebp5hQh9AJ57JssSCqKjQKF0898D3juL7XSg5glj8cAhoG66sQeB74Pn7wBDgIonrHcFsIzQB2ZcunED9gM10o1dCqwKnv8/4IsT6whTqwFrgeRg+n7gP+nmXwl8GzyvCWwBWgP5TvO6bwE/A7vSPd4O5lUN3qM66Zb/B/B68HwF0DHdvHbA6uD5q0C/k/zM74Gn000/DIwOnt8DTAHqx/r/g7z40B6EZMRWd//5+ISZFTKzV4PDL3uAVKCEmcWfZP3Nx5+4+4HgaZEzXPY8YEe6MYB1JyvYQldHZeQkexlCf9GuSTe2htBf/QCPE/qwnmFmC83snqDW/xDaixoE/GRmQ8ysGKE9g0LArOCQyS5C5wnKBq/3T2A5MNbMVprZk+GK8tCn53DgtmDoduD9dIv89/CSuy8HehHay9piZsPN7LxTbPO/3L1EuseJV5Wlf5/XEPpdEPz3xPfp+LxKhALkZDane36A//17eBcYAwwPDlv9I9g7kiyggJCMOPGQx2+B2kBTdy9G6PABhD44o2UTUMrMCqUbq3SyhT10dVSR4PH+yZYLYxtwhNDhs+MqAxuC193s7ve7+3mE9iwGW3AllLu/6O6XABcSOqT0WPB6B4EL030AF/fgogB33+vuv3X36sA1QJ9TnC/4ALjJzKoQOhT3abp5HYGv023/MHdvGWyHEzrMlVHp3+fKwMbg+UZ+/T4dn7cOqHGmP8jdj7j7n929LtAcuJrQIT7JAgoIyQxFCX3o7TKzUsCfov0D3X0NkAY8a2aJZnYpoQ/Us5UYnBgtYGYFgrGPgL+YWdHgw7gPocNZmNnN9r8T8jsJffj+YmaNzaxp8NfufkKHbX5x92PAUKCfmZ0TvEYFM2sXPL86OPFtwB7gl+AR7j2YDWwFXgPGuPuu4DWqAfndfUkwXdvMrghOGP9M6HcV9jUj9Mdgr/FCQucNPgzGPwCeNrOywUnmZ46/T8DrwN1mdqWZxQXbXOd0P8jMLjezesHe6B5CYX02tcsZUEBIZugPFCT01/E0MnZpZUZ0IXT8fjvwPKEPqkNn+ZoLCX2AHn/cDTxK6EN+JTCJ0AniN4LlGwPTzWwfMBLo6e6rgGKEgmAnoUMt24HjVx09Qegw0rTgkNx4QntgEDr5Ox7YB0wFBrv796eo9wNC5xaGpRu7iuDwUiA/8HdCv5/NhE4E//4Ur/m4/d/vQWw7Yf6EoP5vCR2OGhuMP08otOcB84EfgjHcfQah97IfsDt4jSqc3rnAJ4TCYXGwXqZfqSbhWehQpkjOZ6FLTJe4e9T3YLIzMxsFvOTuo0678Jm9blVgFaET3fruSx6gPQjJsYLDODWCQxbtgU7A5zEuKzv4Hvgu1kVIzqdvUktOdi4wgtD18euBh4Lj8nmau/8j1jVI7qBDTCIiEpYOMYmISFi56hBTmTJlvGrVqrEuQ0Qkx5g1a9Y2dy8bbl6uCoiqVauSlpYW6zJERHIMM1tzsnk6xCQiImEpIEREJCwFhIiIhKWAEBGRsBQQIiISlgJCRETCUkCIiEhYCgjgxW9/ZO66XbEuQ0QkW8nzAbHrwGGGTV/L9YMn89dRizl4WPciEREBBQQlCiUytk8ytzauzJDUlXQYkMrUFdtjXZaISMzl+YAAKFYgH3+7oR7D7m+KA7cNncbvP5vPnp+PxLo0EZGYUUCk07xGGUb3TOb+VtUYPmMtbfum8u3in2JdlohITCggTlAwMZ4/XFWXEQ+3oHjBfNz7dho9PpjN9n1ne6tjEZGcRQFxEg0rleDLR1vSq3UtvlmwiTb9UvlizgZ0gyURySsUEKeQmBBHr9bn89WjrahUqhA9h8/hvrfT2LT7YKxLExGJOgVEBGqfW5QRDzXn6asuYPKKbbTtm8qw6Ws5dkx7EyKSeykgIhQfZ9zXqjpjeiVzUYXi/P6z+dz+2jRWb9sf69JERKJCAXGGqpQuzLD7m/L3G+qxcMMe2vVPZUjqCo7+cizWpYmIZCoFRAaYGZ2bVGZcnxRa1SrDX0ct4caXp7Bk855YlyYikmkUEGfh3OIFGHpHEgNva8T6nQe5+sVJ9B23jENH1a5DRHK+qAaEmbU3s6VmttzMngwzv4uZzQseU8ysQbp5JczsEzNbYmaLzezSaNaaUWbGNQ3OY1yfFK6uX54Xv/2RawZOYvbanbEuTUTkrEQtIMwsHhgEdADqAreZWd0TFlsFpLh7feA5YEi6eQOA0e5eB2gALI5WrZmhVOFE+nduxBt3JbH356Pc8PIUnvtqEQcOH411aSIiGRLNPYgmwHJ3X+nuh4HhQKf0C7j7FHc//qf2NKAigJkVA5KB14PlDrv7rijWmmmuqFOOsb2T6dK0Mq9PWkW7/qlMXr4t1mWJiJyxaAZEBWBduun1wdjJ3At8EzyvDmwF3jSz2Wb2mpkVDreSmT1gZmlmlrZ169bMqPusFS2Qj+evq8fwB5oRb0aX16bz5Kfz2H1Qzf9EJOeIZkBYmLGw3ywzs8sJBcQTwVACcDHwsrs3AvYDvzqHAeDuQ9w9yd2TypYte/ZVZ6Jm1UszulcyD6ZU56O0dbTpO4GxCzfHuiwRkYhEMyDWA5XSTVcENp64kJnVB14DOrn79nTrrnf36cH0J4QCI8cpkC+epzpcwOePtKBU4UQeeHcW3Yf9wDY1/xORbC6aATETqGVm1cwsEegMjEy/gJlVBkYAXd192fFxd98MrDOz2sHQlcCiKNYadfUrlmBk95b8ts35jF34E637TuCz2evV/E9Esq2oBYS7HwW6A2MIXYH0kbsvNLNuZtYtWOwZoDQw2MzmmFlaupd4FHjfzOYBDYG/RqvWrJKYEMejV9bi6x4tqVamML0/nMs9b81k4y41/xOR7Mdy01+wSUlJnpaWdvoFs4FfjjlvT1nNP8csJc7gyY4X0KVJZeLiwp26ERGJDjOb5e5J4ebpm9QxEh9n3NOyGmN7J9Oockn++PkCOg+Zxsqt+2JdmogIoICIuUqlCvHuvU34x431Wbx5Dx0GTOSVCWr+JyKxp4DIBsyMWxpXYnyfFFLOL8vfv1nCdYMns2ijmv+JSOwoILKRcsUK8GrXSxjc5WI27/6Za1+axL/HLlXzPxGJCQVENmNmdKxXnnG9U7i24XkM/M9yrnpxErPW7Ih1aSKSxyggsqmShRPpe0tD3rq7MQcP/8JNr0zl2ZEL2X9Izf9EJGsoILK5y2qfw5jeyXRtVoW3pqymXf9UJv6YPXpOiUjupoDIAYrkT+D/dbqIjx68lMT4OLq+PoPHPp7L7gNq/ici0aOAyEGaVCvFqJ6tePiyGoyYvYHW/SYweoGa/4lIdCggcpgC+eJ5vH0dvnikBWWL5Kfbe7N4+P1ZbNn7c6xLE5FcRgGRQ11UoThfdG/BY+1qM37xFtr0TeWTWWr+JyKZRwGRg+WLj+ORy2syqkcrap5ThN99PJc735zJ+p0HYl2aiOQCCohcoOY5Rfj4wUv587UXkrZ6B237pfL2lNUcO6a9CRHJOAVELhEXZ9zZvCpjeyeTVLUUfxq5kFtencoKNf8TkQxSQOQyFUsW4u27G/Ovmxvw45Z9dBgwkUHfLeeImv+JyBlSQORCZsZNl1RkXJ9kWl9wDv8cs5ROL01mwYbdsS5NRHIQBUQudk7RAgzucgmv/OZituw9RKdBk3lh9BJ+PqLmfyJyegqIPKD9ReX5tk8KNzSqwMvfr6DjgInMXK3mfyJyagqIPKJ4oXz88+YGvHNPEw4dPcbNr0zlmS8WsE/N/0TkJBQQeUzy+WUZ2zuZu5pX5d1pa2jXL5UJy9T8T0R+TQGRBxXOn8Cz117IJ90upUC+OO58YwZ9PprDrgOHY12aiGQjCog87JIqpfi6Ryu6X16TkXM20rrvBEbN36R2HSICKCDyvAL54vldu9p80b0F5xYvwMPv/0C392axZY+a/4nkdQoIAeDC84rz+cMteKJ9Hb5bupXWfSfwUdo67U2I5GEKCPmvhPg4HrqsBqN7tqLOucV4/JN5dH19But2qPmfSF6kgJBfqV62CMMfaMZz113E7LU7adsvlTcnr+IXNf8TyVMUEBJWXJzRtVkVxvZJoWn1Uvz5y0Xc/MoUlm/ZG+vSRCSLKCDklCqUKMibdzWm360NWLltPx0HTGLgtz+q+Z9IHqCAkNMyM65vVJHxfVJoc2E5/j1uGdcMnMT89Wr+J5KbRTUgzKy9mS01s+Vm9mSY+V3MbF7wmGJmDU6YH29ms83sq2jWKZEpUyQ/g26/mFe7XsKO/YfpNGgSf/tmsZr/ieRSUQsIM4sHBgEdgLrAbWZW94TFVgEp7l4feA4YcsL8nsDiaNUoGdPuwnMZ1yeFW5Iq8eqElXQYMJHpK7fHuiwRyWTR3INoAix395XufhgYDnRKv4C7T3H3ncHkNKDi8XlmVhG4CngtijVKBhUvmI+/31if9+9rytFjx7h1yDSe/nw+e38+EuvSRCSTRDMgKgDr0k2vD8ZO5l7gm3TT/YHHgVOeDTWzB8wszczStm5V07ms1qJmGcb0SubeltV4f/pa2vVL5bslW2JdlohkgmgGhIUZC3shvZldTiggngimrwa2uPus0/0Qdx/i7knunlS2bNmzqVcyqFBiAn+8ui6fPtScwvkTuPutmfT+cA479qv5n0hOFs2AWA9USjddEdh44kJmVp/QYaRO7n78QHYL4FozW03o0NQVZvZeFGuVTHBx5ZJ81aMlPa6sxZdzN9Km7wS+nLtR7TpEcqhoBsRMoJaZVTOzRKAzMDL9AmZWGRgBdHX3ZcfH3f0pd6/o7lWD9f7j7r+JYq2SSfInxNOnzfl8+WhLKpQsyKMfzOb+d2bxk5r/ieQ4UQsIdz8KdAfGELoS6SN3X2hm3cysW7DYM0BpYLCZzTGztGjVI1nrgvLFGPFQc37fsQ4Tfww1/xs+Y632JkRyEMtN/8MmJSV5WpoyJrtZvW0/T3w6j+mrdtC8Rmn+fkN9KpcuFOuyRAQws1nunhRunr5JLVFXtUxhPri/GX+9vh7z1u+mbf8JvDZxpZr/iWRzCgjJEnFxxu1NKzOuTzLNa5Th+a8Xc8PLU1i6Wc3/RLIrBYRkqfLFC/L6nUkM6NyQdTsOcPXAifQfv4zDR9X8TyS7UUBIljMzOjWswLjeyXSsV57+43/kmoGTmLtuV6xLE5F0FBASM6WL5GdA50a8dkcSuw8e4frBk/nL14s4eFjN/0SyAwWExFzruuUY2yeZzk0qM3TiKtoPSGXqCjX/E4k1BYRkC8UK5OOv19dj2P1NAbht6DSeGjGfPWr+JxIzCgjJVprXKMPonsk8kFydD2eupU3fCYxf9FOsyxLJkxQQku0UTIzn9x0vYMTDLShRMJH73kmjxwez2b7vUKxLE8lTFBCSbTWsVIIvH21J79bn882CTbTuO4Ev5mxQuw6RLKKAkGwtMSGOnq1r8XWPVlQpXZiew+dw39tpbNp9MNalieR6CgjJEc4vV5RPH2rO01ddwOQV22jTN5X3p6/hmNp1iESNAkJyjPg4475W1RnbK4X6FYvzh88WcPtr01i9bX+sSxPJlRQQkuNULl2I9+9ryt9vqMfCDXto1z+VIakrOPqL2nWIZCYFhORIZkbnJpUZ1yeFVrXK8tdRS7jh5Sks3rQn1qWJ5BoKCMnRzi1egKF3XMJLtzdiw86DXDNwEn3HLePQUbXrEDlbCgjJ8cyMq+ufx/g+KVzT4Dxe/PZHrn5xEj+s3Rnr0kRyNAWE5BolCyfS79aGvHlXY/YdOsqNL0/hua8WceDw0ViXJpIjKSAk17m8zjmM7Z1Ml6aVeX3SKtr1T2Xy8m2xLkskx1FASK5UtEA+nr+uHh8+0IyEuDi6vDadJz6Zx+6Dav4nEikFhORqTauX5puereiWUoNPflhPm74TGLtwc6zLEskRFBCS6xXIF8+THerw+cMtKF0kPw+8O4tHhv3A1r1q/idyKgoIyTPqVSzOyO4t+F3b8xm38Cfa9JvAZ7PXq/mfyEkoICRPyRcfR/crajGqZ0uqlylM7w/ncvdbM9mwS83/RE6kgJA8qeY5Rfm4W3P+dE1dpq/cQdu+E3h36mo1/xNJRwEheVZ8nHF3i2qM7Z3MxVVK8scvFtJ5yDRWbt0X69JEsgUFhOR5lUoV4p17mvDPm+qzZPMe2g+YyMvfq/mfSEQBYWY3m1nR4PnTZjbCzC6ObmkiWcfMuDmpEuP7pHB57bK8MHoJ1w2ezKKNav4neVekexB/dPe9ZtYSaAe8Dbx8upXMrL2ZLTWz5Wb2ZJj5XcxsXvCYYmYNgvFKZvadmS02s4Vm1vNMNkoko84pVoBXuybxcpeL2bz7ENe+NIl/jVnKz0fU/E/ynkgD4vj/HVcBL7v7F0DiqVYws3hgENABqAvcZmZ1T1hsFZDi7vWB54AhwfhR4LfufgHQDHgkzLoiUdOhXnnG90mmU8MKvPTdcq56cSKz1uyIdVkiWSrSgNhgZq8CtwCjzCx/BOs2AZa7+0p3PwwMBzqlX8Ddp7j78Zab04CKwfgmd/8heL4XWAxUiLBWkUxRolAi/76lAW/f04Sfjxzjplem8uzIhew/pOZ/kjdEGhC3AGOA9u6+CygFPHaadSoA69JNr+fUH/L3At+cOGhmVYFGwPRwK5nZA2aWZmZpW7duPU1JImcu5fyyjOmdzB3NqvD21NW07ZdK6jL9W5PcL6KAcPcDwBagZTB0FPjxNKtZuJcKu6DZ5YQC4okTxosAnwK93D3s2UJ3H+LuSe6eVLZs2dOUJJIxRfIn8OdOF/HRg5eSP18cd7wxg999PJfdB9T8T3KvSK9i+hOhD++ngqF8wHunWW09UCnddEVgY5jXrg+8BnRy9+3pxvMRCof33X1EJHWKRFvjqqUY1aMVD19Wg89mb6B1vwmMXrAp1mWJREWkh5iuB64F9gO4+0ag6GnWmQnUMrNqZpYIdAZGpl/AzCoDI4Cu7r4s3bgBrwOL3b1vhDWKZIkC+eJ5vH0dvnikBWWL5Kfbez/w0Huz2LL351iXJpKpIg2Iwx7qaOYAZlb4dCu4+1GgO6FzF4uBj9x9oZl1M7NuwWLPAKWBwWY2x8zSgvEWQFfgimB8jpl1jHyzRKLvogrF+aJ7Cx5rV5tvl2yhTd9UPpml5n+Se1gk/5jN7HdALaAN8DfgHmCYuw+MbnlnJikpydPS0k6/oEgmW75lH09+Oo+0NTtpVasMf72+HpVKFYp1WSKnZWaz3D0p7LxI/9oxszZAW0Inn8e4+7jMKzFzKCAklo4dc96bvoYXvlmCA4+3q80dl1YlLi7c9Roi2cOpAiLSk9SFgf+4+2PAUKBgcBJZRAJxccYdl1ZlTO9kkqqW4tkvF3HLq1NZvkXN/yRnivQcRCqQ38wqAOOBu4G3olWUSE5WsWQh3r67Mf++uQE/btlHxwETGfTdco6o+Z/kMJEGhAXfhbgBGOju1xNqnyEiYZgZN15SkfF9Umhd9xz+OWYpnV6azIINu2NdmkjEIg4IM7sU6AJ8HYwlRKckkdyjbNH8DO5yCa/85mK27jtEp0GTeWH0EjX/kxwh0oDoRehLcp8Fl6pWB76LWlUiuUz7i8ozvncKN15cgZe/X0HHAROZuVrN/yR7i/gqpv+uYBYHFDlZ64tY0lVMkhNM+nEbT46Yx/qdB7nj0io83r4ORfJrh1xiIzOuYhpmZsWCq5kWAUvN7HTN+kQkjJa1yjCmVzJ3t6jKu9PW0K5fKt8v3RLrskR+JdJDTHWDPYbrgFFAZULfdBaRDCicP4E/XXMhn3RrTsHEeO56cyZ9PprDzv2HY12ayH9FGhD5gu89XAd84e5HOElnVhGJ3CVVSvJ1j5Y8ekVNRs7ZSJt+E/h63ia165BsIdKAeBVYDRQGUs2sCpDtzkGI5ET5E+L5bdvajOzekvLFC/LIsB948N1ZbNmj5n8SW2d8kvq/K5olBA35sg2dpJac7ugvx3h90ir6jltGYkIcf7yqLjcnVSTU4Fgk82XGSeriZtb3+J3bzOzfhPYmRCQTJcTH8WBKDb7p2YoLyhfj8U/n0fX1GazbcSDWpUkeFOkhpjeAvYRuPXoLocNLb0arKJG8rnrZIgy/vxnPX3cRc9btom2/VN6YtIpfjunchGSdSNt9z3H3hqcbizUdYpLcaOOug/z+s/l8v3QrF1cuwQs31qdWudPdr0skMmd9iAk4aGbH70eNmbUADmZGcSJyaueVKMibdzWm/60NWbVtP1e9OImB3/7I4aNq/ifRFenXN7sB75hZ8WB6J3BndEoSkROZGdc1qkDLWmX485eL+Pe4ZXw9fxP/uKk+9SuWiHV5kktFtAfh7nPdvQFQH6jv7o2AK6JamYj8Spki+Rl4WyOG3pHEzgOHuW7QZP42arGa/0lURHqICQB335OuB1OfKNQjIhFoU7ccY3uncGvjSryaupL2/VOZtnJ7rMuSXOaMAuIEujBbJIaKF8zH326oz7D7mnLMofOQafzhs/ns/flIrEuTXOJsAkLX24lkA81rlmF0r1bc17IaH8xYS9t+qXy3RM3/5OydMiDMbK+Z7Qnz2Aucl0U1ishpFEpM4Omr6/LpQ80pkj+Bu9+aSa/hs9mh5n9yFk4ZEO5e1N2LhXkUdXc1sBfJZhpVLslXPVrS88pafD1/E637TmDk3I1q/icZcjaHmEQkG8qfEE/vNufz5aMtqVSyID0+mM3978xi8241/5Mzo4AQyaXqnFuMEQ+34A8dL2DS8q206TuBD2as1d6EREwBIZKLxccZ9ydXZ3TPZC6sUIynRszn9qHTWbN9f6xLkxxAASGSB1QtU5hh9zXjr9fXY8GG3bTrn8prE1eq+Z+ckgJCJI+IizNub1qZsX2SaVGjDM9/vZgbXp7C0s17Y12aZFMKCJE8pnzxgrx2ZxIv3taIdTsOcPXAifQfv0zN/+RXohoQZtbezJaa2XIzezLM/C5mNi94TDGzBpGuKyIZZ2Zc2+A8xvdJoWO98vQf/yPXDJzEnHW7Yl2aZCNRCwgziwcGAR2AusBtZlb3hMVWASnuXh94DhhyBuuKyFkqVTiRAZ0b8fqdSew+eIQbBk/mL18v4uBhNf+T6O5BNAGWu/tKdz8MDAc6pV/A3ae4+85gchpQMdJ1RSTzXHlBOcb2SaZzk8oMnbiKdv1TmbJiW6zLkhiLZkBUANalm14fjJ3MvcA3Z7qumT1w/F7ZW7duPYtyRfK2YgXy8dfr6/HB/c0wg9uHTuepEfPZo+Z/eVY0AyJct9ew19SZ2eWEAuKJM13X3Ye4e5K7J5UtWzZDhYrI/1xaozSjeybzYHJ1Ppy5ljZ9JzB+0U+xLktiIJoBsR6olG66IrDxxIXMrD7wGtDJ3befyboiEh0FE+N5quMFfP5IC0oWSuS+d9J49IPZbN93KNalSRaKZkDMBGqZWTUzSwQ6AyPTL2BmlYERQFd3X3Ym64pI9NWvWIKR3VvSp835jF4Qav73xZwNateRR0QtINz9KNAdGAMsBj5y94Vm1s3MugWLPQOUBgab2RwzSzvVutGqVUROLjEhjh5X1uLrHq2oUrowPYfP4d6309i462CsS5Mos9z0l0BSUpKnpaXFugyRXOuXY85bU1bzrzFLiY8znupYh9saVyYuTjeYzKnMbJa7J4Wbp29Si0jE4uOMe1tWY0yvZBpUKs4fPlvAbUOnsWqbmv/lRgoIETljlUsX4r17m/LCjfVYtGkP7fun8uqEFRz9Re06chMFhIhkiJlxa+PKjO+TQvL5ZfnbN0u44eUpLN60J9alSSZRQIjIWSlXrABDul7CoNsvZuOug1wzcBJ9xy7l0FG168jpFBAictbMjKvql2dc7xSubXAeL/5nOVe/OIkf1u48/cqSbSkgRCTTlCycSN9bG/Lm3Y3Zf+goN748hf/35SIOHD4a69IkAxQQIpLpLq99DmN6J/ObplV4Y3Ko+d+kH9X8L6dRQIhIVBQtkI/nrruIjx68lIS4OH7z+nQe/2Quuw+q+V9OoYAQkahqUq0U3/RsxUOX1eDTHzbQpu8ExizcHOuyJAIKCBGJugL54nmifR0+f7gFpYvk58F3Z/HI+z+wda+a/2VnCggRyTL1KhZnZPcWPNauNuMW/USbfhMY8cN6Nf/LphQQIpKl8sXH8cjlNRnVsyXVyxSmz0dzuevNmWxQ879sRwEhIjFR85yifNytOc9eU5eZq3fQtu8E3pm6mmPHtDeRXSggRCRm4uOMu1qEmv9dXKUkz3yxkFuHTGXF1n2xLk1QQIhINlCpVCHeuacJ/7ypPks376XDgIkM/n65mv/FmAJCRLIFM+PmpEqM/20KV9Q+h3+MXsp1gyezcOPuWJeWZykgRCRbOadoAV7pegkvd7mYzbsPce1Lk/nnmCX8fETN/7KaAkJEsqUO9cozvk8y1zeqwKDvVnDVixNJW70j1mXlKQoIEcm2ShRK5F83N+Cde5rw85Fj3PzqVJ4duZD9h9T8LysoIEQk20s+vyxjeydz56VVeXvqatr2SyV12dZYl5XrKSBEJEconD+BZ6+9kI8fvJT8+eK4440Z/O7juew6cDjWpeVaCggRyVGSqpZiVI9WPHJ5DT6bvYHWfVP5Zv6mWJeVKykgRCTHKZAvnsfa1WFk9xaUK5afh97/gYfem8WWvT/HurRcRQEhIjnWhecV5/NHWvBE+zp8u2QLbfqm8nHaOjX/yyQKCBHJ0fLFx/HQZTX4pmcrzi9XhMc+mccdb8xg3Y4DsS4tx1NAiEiuUKNsET584FKe63QhP6zZSbv+qbw1eZWa/50FBYSI5BpxcUbXS6sypncyjauW4tkvF3Hzq1NZvmVvrEvLkRQQIpLrVCxZiLfubkzfWxqwYus+Og6YxKDvlnNEzf/OSFQDwszam9lSM1tuZk+GmV/HzKaa2SEz+90J83qb2UIzW2BmH5hZgWjWKiK5i5lxw8UVGdc7hTYXluOfY5bS6aXJLNig5n+RilpAmFk8MAjoANQFbjOzuicstgPoAfzrhHUrBONJ7n4REA90jlatIpJ7lS2an0G3X8yrXS9h675DdBo0mRdGq/lfJKK5B9EEWO7uK939MDAc6JR+AXff4u4zgSNh1k8ACppZAlAI2BjFWkUkl2t34bmM753CTRdX5OXvV9BxwERmrFLzv1OJZkBUANalm14fjJ2Wu28gtFexFtgE7Hb3sZleoYjkKcUL5eOFm+rz3r1NOfzLMW55dSp//HwB+9T8L6xoBoSFGYvoejMzK0lob6MacB5Q2Mx+c5JlHzCzNDNL27pVzbtE5PRa1irD2N7J3NOiGu9NX0PbvhP4bumWWJeV7UQzINYDldJNVyTyw0StgVXuvtXdjwAjgObhFnT3Ie6e5O5JZcuWPauCRSTvKJSYwDPX1OWTbs0plD+Bu9+cSZ8P57Bzv5r/HRfNgJgJ1DKzamaWSOgk88gI110LNDOzQmZmwJXA4ijVKSJ52CVVSvJ1j5b0uKImI+dupE2/CXw9b5PadRDFgHD3o0B3YAyhD/eP3H2hmXUzs24AZnauma0H+gBPm9l6Myvm7tOBT4AfgPlBnUOiVauI5G35E+Lp07Y2Xz7akvLFC/LIsB948N1Z/LQnbzf/s9yUkklJSZ6WlhbrMkQkBzv6yzFen7SKvuOWkZgQx9NXXcAtSZUIHczIfcxslrsnhZunb1KLiKSTEB/Hgyk1GN0rmQvKF+OJT+fzm9ens3Z73mv+p4AQEQmjWpnCDL+/Gc9fdxFz1+2mXf9UXp+0il/yUPM/BYSIyEnExRm/aVaFsb2TaVa9FM99tYibXpnCjz/ljeZ/CggRkdM4r0RB3rirMQM6N2T1tv1c9eIkXvz2Rw4fzd3N/xQQIiIRMDM6NazA+D4ptLvoXPqOW8a1L01i7rpdsS4tahQQIiJnoHSR/Ay8rRFD70hi54HDXD94Mn8btZiDh3Nf8z8FhIhIBrSpW45xfVK4tXElXk1dSYcBqUxbuT3WZWUqBYSISAYVK5CPv91Qn2H3NeWYQ+ch0/jDZ/PZ+3O4BtU5jwJCROQsNa9ZhjG9krm/VTU+mLGWtv1S+c+Sn2Jd1llTQIiIZIKCifH84aq6jHi4BcUK5OOet9LoOXw22/cdinVpGaaAEBHJRA0rleDLR1vSq3UtRs3fRJt+qYycuzFHNv9TQIiIZLLEhDh6tT6frx5tRaVShejxwWzufyeNzbtzVvM/BYSISJTUPrcoIx5qztNXXcCk5dto03cCH8xYm2P2JhQQIiJRFB9n3NeqOmN6JXNRheI8NWI+tw+dzprt+2Nd2mkpIEREskCV0oUZdn9T/nZDPRZsCDX/G5q6Mls3/1NAiIhkETPjtiaVGdcnhZY1y/CXUYu5YfBklm7Ons3/FBAiIlns3OIFGHpHEgNva8T6nQe5euBE+o1blu2a/ykgRERiwMy4psF5jOuTwlX1yjPg2x+5euBE5mSj5n8KCBGRGCpVOJH+nRvxxl1J7P35KDcMnszzXy3KFs3/FBAiItnAFXXKMbZ3Mrc1qcxrk1bRrn8qU1Zsi2lNCggRkWyiaIF8/OX6egx/oBlxBrcPnc5TI+ax+2Bsmv8pIEREsplm1UszulcyD6ZU58OZ62jbbwLjFmV98z8FhIhINlQgXzxPdbiAzx9pQclCidz/Thrdh/3Atixs/qeAEBHJxupXLMHI7i35bZvzGbvwJ9r0ncDnszdkSbsOBYSISDaXmBDHo1fW4useLalapjC9PpzDvW+nsXHXwaj+XAWEiEgOUatcUT7p1pxnrq7L1BXbadsvlfemreFYlNp1KCBERHKQ+DjjnpbVGNs7mYaVSvD05wvoPHQaBw4fzfSflZDprygiIlFXqVQh3r23CR+nrWfWmp0USsz8j3MFhIhIDmVm3NK4Erc0rhSV14/qISYza29mS81suZk9GWZ+HTObamaHzOx3J8wrYWafmNkSM1tsZpdGs1YREfm/orYHYWbxwCCgDbAemGlmI919UbrFdgA9gOvCvMQAYLS732RmiUChaNUqIiK/Fs09iCbAcndf6e6HgeFAp/QLuPsWd58J/J/vkZtZMSAZeD1Y7rC774pirSIicoJoBkQFYF266fXBWCSqA1uBN81stpm9ZmaFwy1oZg+YWZqZpW3duvXsKhYRkf+KZkBYmLFIL9ZNAC4GXnb3RsB+4FfnMADcfYi7J7l7UtmyZTNWqYiI/Eo0A2I9kP7UekVg4xmsu97dpwfTnxAKDBERySLRDIiZQC0zqxacZO4MjIxkRXffDKwzs9rB0JXAolOsIiIimSxqVzG5+1Ez6w6MAeKBN9x9oZl1C+a/YmbnAmlAMeCYmfUC6rr7HuBR4P0gXFYCd0erVhER+TXLio6AWcXMtgJrMrh6GSC2t2/Ketrm3C+vbS9om89UFXcPewI3VwXE2TCzNHdPinUdWUnbnPvlte0FbXNmUrM+EREJSwEhIiJhKSD+Z0isC4gBbXPul9e2F7TNmUbnIEREJCztQYiISFgKCBERCStPBUQE96cwM3sxmD/PzHJ8e48ItrlLsK3zzGyKmTWIRZ2Z6XTbnG65xmb2i5ndlJX1RUMk22xml5nZHDNbaGYTsrrGzBbBv+3iZvalmc0NtjlHf9nWzN4wsy1mtuAk8zP/88vd88SD0Le5VxDqFJsIzCX0re30y3QEviHUaLAZMD3WdWfBNjcHSgbPO+SFbU633H+AUcBNsa47C37PJQi1q6kcTJ8T67qzYJt/D7wQPC9L6P4zibGu/Sy2OZlQT7oFJ5mf6Z9feWkP4rT3pwim3/GQaUAJMyuf1YVmokjuyTHF3XcGk9MINVXMySL5PUOolcunwJasLC5KItnm24ER7r4WQvdiyeIaM1sk2+xAUTMzoAihgDiatWVmHndPJbQNJ5Ppn195KSAiuT/F2dzDIjs60+25l9BfIDnZabfZzCoA1wOvZGFd0RTJ7/l8oKSZfW9ms8zsjiyrLjoi2eaXgAsIdZGeD/R092NZU15MZPrnV9Sa9WVDkdyf4mzuYZEdRbw9ZnY5oYBoGdWKoi+Sbe4PPOHuv4T+uMzxItnmBOASQp2RCwJTzWyauy+LdnFREsk2twPmAFcANYBxZjbRQ81Ac6NM//zKSwERyf0pzuYeFtlRRNtjZvWB14AO7r49i2qLlki2OQkYHoRDGaCjmR1198+zpMLMF+m/7W3uvh/Yb2apQAMgpwZEJNt8N/B3Dx2gX25mq4A6wIysKTHLZfrnV146xBTJ/SlGAncEVwM0A3a7+6asLjQTnXabzawyMALomoP/mkzvtNvs7tXcvaq7VyV0M6qHc3A4QGT/tr8AWplZgpkVApoCi7O4zswUyTavJbTHhJmVA2oTunVAbpXpn195Zg/CI7g/BaErWjoCy4ED5PB7UES4zc8ApYHBwV/URz0Hd8KMcJtzlUi22d0Xm9loYB5wDHjN3cNeLpkTRPh7fg54y8zmEzr88oS759g24Gb2AXAZUMbM1gN/AvJB9D6/1GpDRETCykuHmERE5AwoIEREJCwFhIiIhKWAEBGRsBQQIiISlgJC5DSCjq9z0j1O2iE2A69d9WTdOUViLc98D0LkLBx094axLkIkq2kPQiSDzGy1mb1gZjOCR81gvIqZfRv05P82+LY6ZlbOzD4L7k8w18yaBy8Vb2ZDg3sWjDWzgsHyPcxsUfA6w2O0mZKHKSBETq/gCYeYbk03b4+7NyHUObR/MPYSobbL9YH3gReD8ReBCe7egFBf/4XBeC1gkLtfCOwCbgzGnwQaBa/TLTqbJnJy+ia1yGmY2T53LxJmfDVwhbuvNLN8wGZ3L21m24Dy7n4kGN/k7mXMbCtQ0d0PpXuNqsA4d68VTD8B5HP354PWGPuAz4HP3X1flDdV5P/QHoTI2fGTPD/ZMuEcSvf8F/53bvAqYBChNt2zzEznDCVLKSBEzs6t6f47NXg+hVB3UYAuwKTg+bfAQwBmFm9mxU72omYWB1Ry9++AxwndMvRXezEi0aS/SEROr6CZzUk3Pdrdj1/qmt/MphP6Y+u2YKwH8IaZPQZs5X9dNXsCQ8zsXkJ7Cg8BJ2vHHA+8Z2bFCXUi7efuuzJpe0QionMQIhkUnINIysktpEVORYeYREQkLO1BiIhIWNqDEBGRsBQQIiISlgJCRETCUkCIiEhYCggREQnr/wOD1m2Z/dDY3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Training - Losses v/s Epochs')\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Losses')\n",
    "plt.plot(range(len(losses_train)), losses_train)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
